{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Catln","text":"<p>Catln is a new paradigm of programming languages based around the idea of term rewriting. The language focuses at a high level on the expression of ideas. It can encode ideas that are impossible to describe in most other languages and avoids low-level ideas that other languages leave you no choice but to express. It is most similar to the functional programming languages, but is not technically based around functions.</p>"},{"location":"#language-summary","title":"Language Summary","text":"<p>There are two key pieces to Catln: objects for data and arrows for operations.</p>"},{"location":"#objects","title":"Objects","text":"<p>The fundamental data structure in catln is a named tuple of the format <code>tupleType(argTypeA argNameA = argValA, argTypeB argNameB = argValB, ...)</code>. As the arguments can themselves be tuples, this forms a typed tree structure.</p> <p>This format can be used to represent both code and data. For example, <code>addInts(Int left, Int right)</code> would be a tuple for the addition function. Something like <code>Point(Int x, Int y)</code> could represent a simple data type.</p> <p>With this key structure, it allows for a simple and powerful definition of types: a set of named tuples. Any type feature, no matter how complicated, simply reduces into these sets making them easy to use and reason about.</p> <p>Types can be combined. By taking the union of types, it can represent all the power of many features including enums, sum types, inheritance, and type classes.</p> <p>Types can also intersect by using type properties like <code>List_sorted_length[Int_gt(5)]</code>. The intersection allows for more information about types to be expressed and inferred through type inference. It can then be used for purposes such as optimization or even simple formal verification of assertions. See more about the type theory here.</p>"},{"location":"#arrows","title":"Arrows","text":"<p>While objects support data and functions, arrows enable behavior. Each arrow matches an input type and converts it into an output type such as <code>addInts(Int left, Int right) -&gt; Int</code>. A function definition would create both an object to build the function call and an arrow to execute it.</p> <p>The arrows are very flexible. They can have overlapping input types such as <code>sqrt(Num val) -&gt; Optional[Num]</code> and <code>sqrt(Num_gte(0) val) -&gt; Num</code>. It can even produce a completely different output type from the same input such as <code>sqrt(Num val) -&gt; Complex[Num]</code>. Arrows can also match patterns that have multiple levels such as <code>++(String left, right=++(left=rl, right=rr)) = concat([left, rl, rr])</code>. See more about arrows.</p> <p>That leaves the question of when arrows should be applied and which arrows should be applied. Arrows are applied automatically by the compiler without any specific call by the user. The arrows which are valid in any particular location are determined during type inference based on the inputs and outputs. This leaves the question of which valid arrows to apply for the best performance. Essentially, this makes the programs produced abstracted over the choice of arrows.</p> <p>The goal of deferring the choice of arrows is to only allow you to express information that describes what the result of a function should be, but make it impossible to express information about how to get the result. This provides a clear separation of concerns so you can focus on one level of abstraction or domain of knowledge at a time. Later, you can provide heuristics and manual overrides to address the question of how best to combine levels of abstractions or domains of knowledge by determining these arrow choices. Similarly, you can also abstract over other how choices such as what approximation algorithm or data structure to use.</p> <p>One possible concern is that following different order of applying arrows can result in unexpected behaviors. A well-typed catln program should have any order of applying the arrows result in the same values for the same types. Essentially, different arrows for the same input should be different algorithms and expressions for the same mathematical function. Instead of trusting the assertion, it can actually be tested automatically by using arrow testing. Not only does this allow users to trust the arrow process, but it also provides free testing and code coverage.</p>"},{"location":"#miscellaneous","title":"Miscellaneous","text":"<p>Here are some of the other highlights for features which are possible in the language:</p> <ul> <li>Catln has a different strategy of code organization that is intertwined with documentation. Imagine you were writing a book to teach someone about your code. You want to organize it in the clearest way to explain the concepts to someone else. Now, imagine you want your book to precisely describe the code. The best way to do that is to include all of the code in the book. At this point, your \"book\" is how a code base should look. The purpose for this is to include commentary about not just what code you have, but why you wrote the code you did. See more about documentation or view an example of it in action with the stack documentation.</li> <li>In order to organize code, Catln has modules. When you use a function, you can avoid describing the full module path and instead infer which module your function should be from as part of standard type inference. This means no need for named or qualified imports and large numbers of values can be imported directly into the same scope. It also helps simplify naming. See more about modules.</li> <li>One issue with functional languages is managing state and propagating information tediously down the call stack. While monads can work, combining different monads for different kinds of state adds unnecessary complexity. Catln adds a standard <code>Context</code> type that automatically passes various kinds of state down the call stack. It can be used for semi-global constants, IO, logging, counters, or even event listeners. See more about Context.</li> <li>While Catln can be used to program a normal exectuables, it should be able to describe ideas which are larger than a single executable. For example, it could build both a web client and server, a distributed system, or even an entire cloud architecture with a CloudFormation template. This let's the type checking ensure that all levels of your program work together properly and eliminates bugs. This is best done by moving the compilation and optimization process from the compiler and instead implement it within the standard library using the powerful metaprogramming features Catln provides. See more about language compilation.</li> </ul> <p>While these cover some of the most interesting ideas of the language, many more ideas as well as many further details can be found within the project goals and ideology documentation.</p> <p>You can also view examples of code at the main stack site.</p> <p>The language is currently under development. Only some of the critical language features have been implemented in the compiler so code written in the language is still somewhat limited.</p> <p>Right now, the team developing Catln consists of @zachgk and @githubpradeep.</p> <p>Feel free to reach out to us on our Slack or GitHub Discussions if you have any thoughts, ideas, questions, suggestions, feedback, or concerns about the language. You can also join to discuss programming language design, if you are interested in contributing to the language, or are interested in joining the Catln team.</p>"},{"location":"building/","title":"Getting Started","text":""},{"location":"building/#prerequisites","title":"Prerequisites","text":"<p>There are three main prerequisites for installing Catln:</p> <ul> <li>Haskell GHC comiler</li> <li>Haskell Stack, to build the compiler and manage Haskell dependencies.</li> <li>LLVM 8 which can be installed through a package manager such as <code>brew install llvm@8</code> or <code>apt install llvm-8</code></li> </ul>"},{"location":"building/#installation","title":"Installation","text":"<p>To install the Catln compiler, begin by checking out the Catln repository. I will refer to this directory as <code>$CATLN_HOME</code>. Then, you can install it by running:</p> <pre><code>cd $CATLN_HOME\nstack install\nmkdir -p ~/.catln\nln -s $CATLN_HOME/stack ~/.catln/stack\n</code></pre> <p>This installs Catln to <code>~/.local/bin/catln</code>. You may have to add <code>~/.local/bin</code> to your path in order to run catln.</p> <p>It also sets up your home directory to reference the installed catln stack, to find core libraries and packages.</p> <p>You can determine the available options for catln by running <code>catln --help</code>.</p> <p>If you want to run the docs, you will also need to build the docs site as well. Install <code>npm</code> and then run the following:</p> <pre><code>cd $CATLN_HOME/webdocs\nnpm install\nnpm run build\n</code></pre>"},{"location":"building/#running","title":"Running","text":"<p>Now that it is installed, the compiler should be available in your path. You can test this by running <code>catln --help</code>.</p> <p>There are several subcommands available within the compiler.</p> <p>You can use <code>catln run FILEPATH [FUN]</code> to run a particular file. It will expect to find a definition of <code>[FUN]</code> within that file to run. Passing <code>[FUN]</code> is optional and the default is the function named <code>main</code>. This command must be run from the directory of <code>$CATLN_HOME</code>.</p> <p>You can use <code>catln build FILEPATH [FUN]</code> to build a particular file. It will expect to find a definition of <code>[FUN]</code> within that file defining what to build. Passing <code>[FUN]</code> is optional and the default is the function named <code>main</code>. This command must be run from the directory of <code>$CATLN_HOME</code>.</p> <p>You can use <code>catln doc PATH</code> to run the doc server on port <code>8080</code>. It will serve all <code>.ct</code> files located recursively in the <code>PATH</code> if it is a directory, or just <code>PATH</code> if not. It will also include all dependencies in the doc build. This command must be run from the directory of <code>$CATLN_HOME</code> and requires some additional installation (see above).</p> <p>There are also several additional options to run in the building for development document.</p>"},{"location":"syntax/","title":"Syntax","text":"<p>Here is a basic guide to the currently implemented syntax and features for Catln. You can view examples of the syntax inside the stack site. The raw files are available for both the stack files and test files.</p>"},{"location":"syntax/#comments","title":"Comments","text":"<p>A comment is done using by prefixing a line with <code>#</code>. The comment includes all lines that are indented after it as well. The contents of the comment are treated as markdown so all markdown syntax (headings, bold, links, etc.) will work in the comment.</p> <pre><code># This is a comment\n\n   # Markdown title inside the comment.\n</code></pre>"},{"location":"syntax/#declarations","title":"Declarations","text":"<p>To declare a value, you can write an equality:</p> <pre><code># Type inferred value\nx = 5\n\n# Explicitly Typed value\nInteger y = 3\n</code></pre> <p>Right now, only integers and booleans are supported. The booleans are written as <code>True</code>, <code>False</code>, or <code>Boolean</code>.</p> <p>A function is written with an equals followed by the arguments:</p> <pre><code>double(Integer val) = val + val\n\n# With a return type\ndouble2(Integer val) -&gt; Integer = val + val\n\n# Call the function by passing in the arguments\nresult = double(val=5)\n\n# Call the function while attempting to infer the argument name\nresult2 = double(5)\n\n# Functions can also be defined in a method format\n# It behaves like a function with the caller as an argument called \"this\"\nInteger.double3 = this + this\nresult3 = 5.double3\n</code></pre> <p>Catln supports the following operators following the typical operator precedence:</p> <ul> <li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code>, <code>-</code> (unary opposite)</li> <li>Comparison: <code>&lt;=</code> <code>&gt;=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>==</code>, <code>!=</code></li> <li>Boolean: <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>~</code> (unary negation)</li> <li>Parenthesis</li> </ul> <p>By convention, both values and functions should begin with a lowercase letter and use camel case.</p> <p>For longer functions, they can be split over multiple lines. The earlier lines can themselves be value declarations or inner function declarations. The last line should be an expression that is interpreted as the return value.</p> <pre><code>sumEqProduct(Integer a, Integer b) =\n    sumVal = a + b\n    prodVal = a * b\n    sumVal == prodVal\n\nresult = sumEqProduct(a=2, b=2)\n</code></pre> <p>In addition, you can also add compiler annotations to a multi-line declaration. The compiler annotations should each have their own line and can be recognized as they start with a <code>#</code> sign.</p> <pre><code>doublePositive(Integer val) =\n    #assert(test=val&gt;0)\n    val + val\n</code></pre> <p>You can also add a conditional guard to a declaration. You can give either an if or an else guard. The else guard only activates if none of the if conditions execute.</p> <pre><code>abs(Integer x) if x &gt;= 0 = x\nabs(Integer x) else = x\n\n# With return values\nabs2(Integer x) if x &gt;= 0 -&gt; Integer = x\nabs2(Integer x) else -&gt; Integer = x\n</code></pre> <p>You can also provide an inline ternary statement.</p> <pre><code>abs(Integer x) = if x &gt;= 0 then x else -1 * x\n</code></pre> <p>You can use the match statement to pattern match against a value. With the match statement, the order given does not matter. Any matching conditions can be executed for a particular value. If there is overlap among the cases, they will eventually be validated by arrow testing.</p> <pre><code>abs(Integer x) = match x of\n                  x2 if x &gt;= 0 =&gt; x2\n                  x2 else =&gt; x2 * -1\n</code></pre> <p>The case statement, unlike the match statement, executes the expressions in order. Each succeeding statement will only be tried if the preceeding ones all fail.</p> <pre><code>abs(Integer x) = case x of\n                  x2 if x &gt;= 0 =&gt; x2\n                  x2 =&gt; x2 * -1\n</code></pre>"},{"location":"syntax/#types","title":"Types","text":"<p>A new type object can be created through a data declaration. By convention, type objects should begin with a capital letter and use camel case.</p> <pre><code>data Pair(Int a, Int b)\n\n# Create a pair\nmyPair = Pair(a=1, b=2)\n\n# Pattern match against a data object in a declaration\nfst(val=Pair(a, b)) = a\nsnd(val=Pair(a, b)) = b\n\n# A pair with a type parameter\ndata Pair2[$N]($N a, $N b)\n</code></pre> <p>To create a union type, use a class declaration. Each of the elements of the union (Red, Green, and Blue here) will be treated as new type objects if they don't already exist.</p> <pre><code>class Stoplight = Red | Greed | Blue\n\n# A class with a type parameter and argument\nclass Maybe[$T] = Just[$T]($T val) | Nothing\n\n# A union that directly uses $T without creating a new object\n# In this instance, it is not a sum type but a true union type\nclass Maybe2[$T] = $T | Nothing\n</code></pre> <p>You can also define annotations similarly to data objects:</p> <pre><code>annot #assert(Boolean test)\n</code></pre>"},{"location":"syntax/#programs","title":"Programs","text":"<p>There are two operations that Catln can execute on a program: running and building.</p>"},{"location":"syntax/#running","title":"Running","text":"<p>A runnable program is one which can be executed. It is most similar to what you would see in most programming languages. It has two signatures:</p> <pre><code>f -&gt; Showable\nf{IO io} -&gt; IO\n</code></pre>"},{"location":"syntax/#build-main","title":"Build - main","text":"<p>A full build produces a program that does not necessarily consist of just a single executable. The only current example is the web test which produces a single page website. In the future, other types of builds will be created.</p> <p>In addition, some runnable functions can also be built. This corresponds to applying the <code>llvm</code> macro to compile the runnable into a result. The build results will be created in the <code>build</code> directory.</p> <p>The possible signatures to build are:</p> <pre><code>f{IO io} -&gt; IO\nf -&gt; CatlnResult\nf{IO io} -&gt; CatlnResult\n</code></pre>"},{"location":"comingFrom/","title":"Coming From","text":"<p>When learning a new language, many of the ideas and concepts we know from existing languages transfer over. However, often times there are differences where old languages don't relate to new ones.</p> <p>This section contains some specific parts to keep in mind for developers from different programming language backgrounds. It isn't a complete tutorial, just a highlighted list of some common difficulties or misunderstandings that you may have.</p>"},{"location":"comingFrom/haskell/","title":"Coming From Haskell","text":"<p>This is a short guide of things to keep in mind when using Catln for programmers from a Haskell background.</p>"},{"location":"comingFrom/haskell/#arrow-notation","title":"Arrow notation","text":"<p>Both Haskell and Catln use an arrow notation <code>a -&gt; b</code> in types. However, they have different meanings.</p> <p>In Haskell, <code>a -&gt; b</code> would be read that you have a function which accepts an argument of type <code>a</code> and returns a value of type <code>b</code>.</p> <p>If you see <code>a -&gt; b</code> in Catln, it means that something of type <code>a</code> can be converted into something of type <code>b</code>. For example, <code>List -&gt; Set</code> says that any list can be converted into a set.</p> <p>To describe a function type, it would look like <code>f(a) -&gt; b</code> where <code>f</code> is the function name or the argument name of the function. One benefit of this system is that having a name for <code>f</code> makes it easy to describe the function. It is also possible to have classes of functions such as <code>Monotonic f(a) -&gt; b</code> where the class <code>Monotonic</code> is applied to <code>f</code>.</p>"},{"location":"comingFrom/java/","title":"Coming From Java","text":"<p>This is a short guide of things to keep in mind when using Catln for programmers from a Java background.</p>"},{"location":"comingFrom/java/#inheritance","title":"Inheritance","text":"<p>One of the first differences when seeing Catln is that it doesn't match up exactly with the idea of inheritance. However, Catln uses different language but can achieve most of the same effect. However, there are two key areas to keep in mind.</p> <p>First, it resolves ambiguities with regard to classes. Imagine you have the following two classes:</p> <pre><code>public class Parent {}\npublic class Child extends Parent {}\n</code></pre> <p>Let's say that you have a <code>Parent</code> object. This is actually ambiguous because there are two different meanings. You could be referring to the group of classes that are <code>Parent-like</code>, or the specific member of the group which is the <code>Parent-exact</code>. Without different ways to refer to these different concepts, code can't tell whether you want the group of classes or the exact class.</p> <p>To handle this situation, there is a theory of Covariance and Contravariance. However, Catln resolves it by adding an additional restriction compared to OOP: all classes must be abstract or final. Then, it is clear.</p> <p>In Catln terms, both Java interfaces and Java abstract classes can be implemented using Catln classes. Java final classes can be implemented as Catln <code>data</code>.</p> <p>The second restriction comes from the use of overloading methods. In Catln, every function is considered to be a true statement. If it were allowed to overload a function, it would mean that the function may or may not be true. This is fairly useless as far as statements go.</p> <p>So, overloading is only allowed when both definitions are true (and usually equal for all function inputs). For example, imagine you have a set and want all the values in a range. For any set, you can list all the elements and filter to only the ones in the range. If it were a tree set, you could use tree operations to find the range. But, the general set operation would still work even though it would be less efficient.</p> <p>In cases where you need behavior which can be overrided, consider putting the default behavior in a supplementary class. Users can extend the supplementary class to get the default behavior or extend only the main class to get their own behavior.</p>"},{"location":"contrib/building/","title":"Building","text":""},{"location":"contrib/building/#building-for-development","title":"Building for development","text":"<p>To build the compiler, you can run <code>stack build --pedantic</code> from <code>$CATLN_HOME</code>.</p> <p>Once it is built, you can execute the compiler on a Catln file by running <code>stack exec catln test/code/arith.ct</code>.</p> <p>You can also use a repl as follows:</p> <pre><code># Start stack repl\nstack repl\n\n# run command in repl\n# the ... represents all files in the program\n# process run the file\n*...&gt; process \"test/code/arith.ct\"\n</code></pre> <p>A repl that also keeps a stack trace for finding the source of errors can also be run. Use <code>make errRepl</code> or <code>stack ghci --profile catln --ghci-options \"-fexternal-interpreter -prof\" --test</code>. This can take a few hours to build the first time it is run.</p>"},{"location":"contrib/building/#tests","title":"Tests","text":"<p>To run the Catln test suite, execute <code>stack test --pedantic</code>.</p> <p>You can also load the tests within the repl:</p> <pre><code># Start stack repl\nstack repl\n\n# Load the main test file in the repl\n*...&gt; :l test/Spec\n\n# Execute the main test suite command including the full test suite\n*...&gt; main\n\n# Execute the \"arith\" test located in \"test/code/arith.ct\"\n*...&gt; mt \"arith\"\n\n# Execute the custom untracked test file located at \"test/test.ct\"\n# This is useful during development\n# Unlike the main tests, this one does not include the core library\n*...&gt; test\n</code></pre> <p>To rerun a property test, use:</p> <pre><code>stack test --ta '--hedgehog-replay \"1:a2 Seed 10220680066336263475 12463056215188250787\" -p \"propCompactIdempotent\"'\n</code></pre> <p>You can also use a retest similar to that defined in TypesTests from the REPL.</p>"},{"location":"contrib/building/#webdocs","title":"Webdocs","text":"<p>Webdocs can be built using the standard way described in the main building document. In addition, another strategy for building webdocs can be done during development. It benefits from being somewhat faster to run, being runnable from the REPL without building code, and featuring live-reload for making changes to the webdocs.</p> <p>In one command line, start the webdocs server using:</p> <pre><code>cd webdocs\nnpm start\n</code></pre> <p>Then, in another command line, run the webdocs API serving:</p> <pre><code># See above for description of stack repl\nstack repl\n*...&gt; :l test/Spec\n\n# Run docs for test name \"id\" located in \"test/code/id.ct\"\n*..&gt; mtd \"id\"\n</code></pre> <p>Then, the API server should be available at <code>localhost:31204</code>.</p> <p>The local testing site is available at <code>localhost:3000</code>.</p>"},{"location":"contrib/building/#formatting","title":"Formatting","text":"<p>As part of the development, we use stylish-haskell to ensure that code has a consistent appearance. The formatter can be run through:</p> <pre><code>make format\n</code></pre> <p>We also format the catln code using the catln formatter. This can be done with:</p> <pre><code>make ctformat\n</code></pre>"},{"location":"contrib/building/#profiling","title":"Profiling","text":"<p>To profile, it can be done by running <code>stack test --profile</code>. This produces the file <code>catln-test.prof</code>.</p> <p>To make it easier to understand, there are tools to provide visualizations of the results. One can be done by running <code>profiteur catln-tet.prof</code>. This produces a file <code>catln-test.prof.html</code> which can be viewed.</p>"},{"location":"contrib/organization/","title":"Code Organization","text":"<p>The main executable is located in app. The tests are located in test. The rest of the code is located within the src directory.</p> <p>For understanding the code, it is best to start by looking at Syntax. This file contains main class definitions to describe the syntax of the program. The main piece of syntax is called <code>RawPrgm m</code> or <code>Prgm m</code> that includes some amount of additional metadata <code>m</code>.</p> <p>Most of the code works similar to a pipeline. It converts the program code from one format to another and from one metadata type to another until the end result is finally used. The pipeline flows as follows:</p> <ol> <li>Parser - <code>String -&gt; RawPrgm PreTyped</code>. This parsers the syntax of the string text file into a raw format.</li> <li>Desugar - <code>RawPrgm PreTyped -&gt; Prgm PreTyped</code>. The desugar process removes various kinds of syntactic sugar and converts the code from the more user friendly external format into a more minimal format better suited towards optimization.</li> <li>TypeCheck - <code>Prgm PreTyped -&gt; Prgm Typed</code>. The typechecking will execute typechecking and type inference to ensure that all types are known for all of the functions.</li> <li>TreeBuild - <code>Prgm Typed -&gt; ResArrowTree</code>. Once typechecked, the implicit conversions are chosen and the code is formed into a single tree. This stage currently lives within the Eval stage as it requires knowledge about the primitive instructions and data types available.</li> <li>Eval - <code>Prgm Typed -&gt; Val</code>. The evaluator is an interpreter that will execute the code and produce final values. For building or compiling programs, it is executed through evaluation as a macro.</li> </ol>"},{"location":"philosophy/","title":"Design","text":"<p>Here are a number of the core ideas and directives that drive the project. Many of these are not currently implemented, but will hopefully be implemented in the future.</p>"},{"location":"philosophy/#sub-pages","title":"Sub-Pages","text":"<ul> <li>basics - Discusses the basic elements within the type system.</li> <li>typeTheory - Introduction to the ideas behind the type theory.</li> <li>choice - The ability to write code that abstracts over lower level choices and design decisions and express information joining levels of abstraction.</li> <li>metaprogramming - Discusses support for meta programming.<ul> <li>macros - For a strongly typed language, you must either support unsafe features, require duplicate code, or support macros. This describes the macro usage within the language.</li> <li>Language Based Compilation - Move logic from the compiler to the language and library for greater capabilities, faster upgrades, more customization, and ease of use.</li> <li>compilerErrorStatement - Declare error messages from within function definitions themselves.</li> </ul> </li> <li>testing - General introduction to the testing philosophy and techniques.<ul> <li>arrowTesting - Automatic testing to ensure that overlapping function definitions should have the same result. It makes the use of multiple return values and implicit conversions safe.</li> <li>assertions - Using assertions both for property testing and to declare properties.</li> </ul> </li> <li>Context - An additional calling convention for passing data down through the call stack. This can be used for easier while still manageable state management.</li> <li>optimization - Various aspects about performance optimization.</li> <li>syntacticSugar - Various forms of syntactic sugar to add.</li> <li>documentation - Documentation organization.<ul> <li>naming - Thoughts on naming and some language specific guidance for good naming.</li> </ul> </li> <li>modules - Module and code scoping systems.</li> <li>debugging - Tools for debugging.</li> </ul>"},{"location":"philosophy/arrowTesting/","title":"Arrow Testing","text":"<p>One of the most important ideas in the language is arrow testing. The general idea behind arrow testing is that when there is any kind of overlap in definitions, they must have equal results given the same inputs. This can be verified either using property testing and automatically providing inputs to the function or through theorem proving methods.</p> <p>The simplest use case for arrow testing is inheritance. In Java for example, you can override a method in a subclass compared to the base class. However, the overrided method can differ in behavior from the base class which means you can never be sure that the behavior of method does not change. Arrow testing, however, requires that the overrided method must always return the same result. This means that the overriding is only useful when there is a more performant way to calculate the result in the child (although it would be more and less specific types here).</p> <p>This is even more important in the case of multiple inheritance. The typical problem is that it is entirely unclear in the case of both parents having the same method which one should be used. Arrow testing guarantees that they must be equal so either choice would be equally valid. In this case, some simple heuristics can be used to try to select the best choice or it can be manually specified.</p> <p>Another benefit of arrow testing is that various methods can be tested by providing multiple definitions. For example, a more complicated fib method can be combined with the simpler variation to verify that the complicated one is still correct. It could also be used (along with an annotation to indicate tests) to write simple unit tests instead of a simple testing framework. Lastly, it can be used to describe invariates such as the Functor laws to help verify that class instances are correctly defined.</p>"},{"location":"philosophy/arrowTesting/#implicit-arrow-testing","title":"Implicit Arrow Testing","text":"<p>But, the greatest benefit of arrow testing comes about in combination with implicit conversions. Similarly to composing diagrams in category theory, any two paths going from the same start node to the same end node must be equal. So, the conversions themselves can be converted by following different conversion paths to the same result. It can also test functions. For any combination of <code>implict -&gt; method -&gt; implicit</code> that can get the same type, it should be equal and can be tested.</p> <p>As an example, consider a list type. You can have multiple kinds of lists such as an arrayList or a linkedList. Since they are both lists, you could implicitly convert between the two types of lists. So, if you start with a value in an arrayList, convert it into a linkedList, and take the length, it should be equivalent to taking the length directly. Like this, you can use the comparison to test almost the entirety of the list class and the methods within it. It is basically a free test suite, although additional tests may still be beneficial. And, those additional tests would only need to test one kind of list and would be automatically applied to all of them.</p> <p>Another major benefit is also introduced from this. It helps ensure that the APIs are consistent. For example, you could have an implicit between an Optional type and a List type where the list uses zero elements to represent Nothing and a singleton list to represent a value. If both these types have a map function, it would mean that their map functions must be equivalent. This is true even if the implicit is only one way (Optional -&gt; List) as that direction can be tested even if the reverse is not.</p>"},{"location":"philosophy/arrowTesting/#implementation","title":"Implementation","text":"<p>Imagine you have a multi-level tuple <code>++('a', ++('b', 'c'))</code>. Then, you are able to follow implicit rules and substitute the implicit rules for various sublevels of the tuples. Then, you could evaluate it to:</p> <pre><code>++('a', ++('b', 'c')) -&gt; ++('a', \"bc\") -&gt; \"abc\"\n\n++('a', ++('b', 'c')) -&gt; concat(['a', 'b', 'c']) -&gt; \"abc\"\n</code></pre> <p>Essentially, apply all possible implicit rules nondeterministically to find all possible values that would be generated during the evaluation. For all of the generated values (including initial, intermediate, and final values), all ones with the same tuple types at all levels must be equal.</p> <p>An easy way to test this is to find a map from the value types to the produced values. Then, check to see that the produced values for each type are all equal. If not, show an informative error to the user.</p> <p>Given this implementation to execute arrow testing, the remaining problem is how to generate these multi-lelvel tuples. There will probably be different areas to generate for different situations.</p>"},{"location":"philosophy/assertions/","title":"Assertions","text":"<p>In many languages, the source files and the test files are typically kept separate. With arrow testing, it is possible to have a reasonable amount of testing integrated naturally into the code.</p> <p>To expand on this, assertions can be integrated into the code as well. The effect of an assertion can be assumed to limit the domain of a value, often relative to other values and inputs within the function. There are two major purposes to this.</p> <p>First, the assertions are used for testing. In a specific test function, it would act like a normal unit test or integration test. When integrated into the main code functions, it behaves like property testing. Those tests can run one of two ways. If it is possible to prove the assertion using the type system's type properties, it is the best assurance. Otherwise, a testing Engine can attempt to provide inputs that are random and edge cases to verify that the assertion is always true. See the testing document for more details.</p> <p>The other key benefit of the assertions is that they can serve as a key input with respect to type properties. Because the assertion is true, it can be used to trigger various implications. From there, the type inference engine can use it to better understand the surrounding code. So, the assertions can be used to give \"type hints\" to help make up for holes in the type inference process.</p>"},{"location":"philosophy/basics/","title":"Basics","text":"<p>The fundamental data structure in catln is a named tuple of the format <code>tupleType(argTypeA argNameA = argValA, argTypeB argNameB = argValB, ...)</code>. As the arguments can themselves be tuples, this forms a typed tree structure.</p> <p>This format can be used to represent both code and data. For example, <code>addInts(Int left, Int right)</code> would be a tuple for the addition function. Something like <code>Point(Int x, Int y)</code> could represent a simple data type.</p> <p>In addition to the data tuples, there are also arrows which convert one tuple format to another. For example, there could be an arrow <code>addInts(Int left, Int right) -&gt; Int</code> that applies the integer addition operation. These arrows can match complicated patterns involving the data themselves, and even patterns involving the composition of multiple functions. Arrows can even take the same input tuple to multiple output tuples.</p>"},{"location":"philosophy/basics/#objects","title":"Objects","text":"<p>In Catln, objects are used to define the allowed types of tuples. Each object behaves similar to a product type or a cross product. To give some examples, let me explain the two ways objects can be created: function and data types.</p> <p>A function declaration looks like:</p> <pre><code>addInts(Int left, Int right) -&gt; Int\n</code></pre> <p>This is an object that consists of a single product with two arguments. For now, ignore the return value.</p> <p>If you overload a function, it creates two objects that share the same name, but with different arguments.</p> <pre><code>add(Int left, Int right) -&gt; Int\nadd(Float left, Float right) -&gt; Float\n</code></pre> <p>It is also possible to create a data type as an object. Each data declaration also creates a single object.</p> <pre><code>data ComplexInt(Int real, Int irrational)\n</code></pre>"},{"location":"philosophy/basics/#type-classes","title":"Type Classes","text":"<p>Along with objects, Catln also has type classes. Each type class corresponds to the combination of one or more objects. If each object is a product, the type class is a union of products. The type class can also be thought of as a boolean property where a tuple has a set of type classes that it matches.</p> <p>Type classes can be used to represent common properties and behaviors across multiple objects. It is analogous to enums, interfaces, and parent classes in object oriented classes. It also corresponds to both sum types and type classes in Haskell.</p> <p>There are two different kinds of type classes: sealed and unsealed. A sealed type class is one where the allowed objects are fixed at creation. One example is this optional int type:</p> <pre><code>data OptionalInt = Some(Int val) | Nothing\n</code></pre> <p>Because they are fixed, sealed type classes can be used for pattern matching and enums.</p> <p>Unsealed type classes are used to represent unions that can be extended later. This corresponds to the interface or type class.</p> <pre><code>class Show\nshow(Show s) -&gt; String\n\nevery String isa Show\nshow(String s) = s\n\nevery Boolean isa Show\nshow(True) = \"True\"\nshow(False) = \"False\"\n</code></pre> <p>Here, a new class called show is declared.</p> <p>Then, the function <code>show</code> is declared which should show all items of type show. Note that this syntax is a generic function declaration. The compiler will verify that all appropriate definitions exist to fulfill a claim made within the declaration. So, this syntax can also be used outside of type classes.</p> <p>Next, two instances of the <code>Show</code> class are added: String and Boolean. Each must also fulfill the <code>show</code> definition as they have been added to the domain of the function. They are described using an <code>every _ isa _</code> statement.</p> <p>It is also possible to have a class extend another class. For example, all classes which are comparable also have the equality class. No special syntax is required to handle function as it merely requires that all appropriate declarations are fulfilled.</p> <p>Just like Haskell, types are added to classes separately from their construction. For your own types, this is not important. However, this feature becomes useful when working with dependencies that are more difficult to change. You can add additional type classes to types within those dependencies, including types within the standard library.</p> <p>Another idea worth mentioning is that type classes, sealed or unsealed, represent general union types. The only effect that sealing a type class has is to throw a compiler error if you try to extend one. The implementation, otherwise, is completely identical.</p>"},{"location":"philosophy/basics/#generics","title":"Generics","text":"<p>It is also possible to have function, data, and classes that depend on other types. For example:</p> <pre><code>add[Numeric $T]($T left, $T right) -&gt; $T\n\ndata Complex[Numeric $T]($T real, $T irrational)\n\nclass Optional[$T] = Some($T val) | Nothing\n</code></pre> <p>The type variables are written within angle braces, begin with a dollar sign and a capital letter. Here, several of them require that the type variable be instances of the Numeric type class.</p> <p>They behave fairly straightforwardly in that they simply increase what the object or type is capable of matching. The type variable directly match only objects, but matching all objects within a type class is equivalent to matching the type class itself. They also can help infer outputs when the output is the result of the type variable. See macros for more information.</p>"},{"location":"philosophy/basics/#type-properties","title":"Type Properties","text":"<p>The other key design feature of the type system is type properties. Type properties are attached to types or typeclasses and add additional metadata about them. The goal of the type checker is to infer the type properties when possible and use them to better inform the choice of objects and arrows.</p> <p>For example, the <code>List</code> type might have a property <code>List_length[Int]</code>. This can be used to change the return value of the <code>head</code> function depending on the inferred length of the list:</p> <pre><code>List[$T].head -&gt; Optional[$T]\nList_length[Int_gt(0)][$T].head -&gt; $T\n</code></pre> <p>Now, calling the head function on a list can give you either an optional or direct value. Therefore, you can directly use the value when it is known to be safe without writing unnecessary code to handle an impossible <code>Nothing</code> condition. When the length can't be inferred, then you do have to handle the very real <code>Nothing</code> condition. If it can be inferred, you can still treat it as an <code>Optional</code>, but it will throw a compiler warning because the <code>Nothing</code> handling code is unreachable.</p> <p>For all lists, you can also assume that the length is at least 0. This leads to the relationship:</p> <pre><code>List -&gt; List_length[Int_gt(0)]\n</code></pre> <p>In addition, it is important to describe how various functions and arrows affect the type properties. For example:</p> <pre><code>List_length[Int_gt(a)].concat(List_length[Int_gt(b)]) -&gt; List_length[Int_gt(a+b)]\n</code></pre> <p>Rules defined like this can enable the compiler to infer type properties even when they are not explicitly declared. Higher level functions can use these rules to automatically compute the type properties of return values.</p> <p>To clarify, type properties can accept an input either as a type or an expression. Types, like usual, are provided using <code>[]</code> while expressions use <code>()</code>. When they are compared, it checks using the type of the expression and the direct types.</p> <p>I recommend thinking about type properties in terms of sets of values. Each object and type class represent a set of possible values. Then, the type properties further restrict the set of possible values of the type into a subset of the original. This allows more information of what is known about various data types to be propagated through the program and used for both static analysis and optimization.</p> <p>Type properties exist in a somewhat similar space to refinement types. The key difference is that they are defined entirely within Catln itself while most refinement type systems use an external tool such as Z3. That means that the external tool would be limited to operating on preset domains of knowledge while type properties can grow as more things are implemented in the language.</p> <p>Another difference is that it is also possible to use type properties for more abstract or computed values. You could add a <code>String_hasPassword[Boolean]</code> property to help check whether it is possible that a password is contained in a String. This could be used for security to check the values before saving in a database or sending to a browser. Another possible property would be something like <code>HTML_classList[List[$T=String]]</code>. This property can be used to compute the possible classes an HTML element could have, useful for pruning unused parts of the CSS file.</p>"},{"location":"philosophy/basics/#arrows","title":"Arrows","text":"<p>An arrow represents a conversion from one data tuple to an expression (this is the same as one data tuple to another data tuple). Each arrow consists of the Object it applies to, the resulting expression (declarations are written as arrows without an expression), a conditional guard, and possible compiler annotations. See more.</p> <p>The objects can be thought of like a pattern that is matched against tuples. Each object then corresponds to zero or more arrows that are applied if the pattern matches (zero for pure data type). So, there is also no problem of having multiple objects match the same tuple since it just means all the arrows from all matching objects apply.</p> <p>Arrows are created by providing either a declaration or definition. A declaration features the object (or pattern) on the left side and the resulting type on the right side. It states that there should exist a definition for this type, but doesn't explicitly state what it is.</p> <pre><code>show(Show s) -&gt; String\n</code></pre> <p>A definition, unlike a declaration, does return a value. The expression on the right side of the <code>=</code> is the tuple that is returned as a result of the arrow. It can involve sub-definitions and sub-declarations as well as arguments from the pattern matching on the left hand side of the <code>=</code>.</p> <pre><code>not(val=True) = False\nnot(val=False) = True\n</code></pre> <p>Both a declaration and definition create two components: an object and an arrow.</p>"},{"location":"philosophy/basics/#true-statements","title":"True Statements","text":"<p>The other major syntactic structure for effective type properties is an implication. The purpose of the implication is to support true statements. For example, consider this simple absolute value function:</p> <pre><code>abs(Int i) = if i &gt;= 0\n               then i\n               else -i\n</code></pre> <p>This definition has a conditional \"if\" statement that branches the code based on the definition of <code>i</code>. However branching the code is also the process of learning about <code>i</code>. While <code>i</code> can be any integer in the function at large, we know it is limited to <code>Int_gte(0)</code> within the then branch. The reason we know this is because within the branch, we can further assume the statement <code>i &gt;= 0</code> is true. Likewise, we know within the else branch that <code>i</code> is of type <code>Int_lt(0)</code> because <code>not(i &gt;= 0)</code> is true.</p> <p>To convert between expressions and the type properties that they imply are implication statements. Here are some examples:</p> <pre><code># the left part of a true boolean expression is also a true statement\noperator&amp;(Boolean l) .: l\n\n# the right part of a true boolean expression is also a true statement\noperator&amp;(Boolean r) .: r\n\n# the left side of a &gt;= has type Int_gte(r)\noperator&gt;=(Int l, Int r) .: l :: Int_gte(r)\n\n# the right side of a &gt;= has type Int_lt(l)\noperator&gt;=(Int l, Int r) .: r :: Int_lt(l)\n</code></pre> <p>These implications are used to adjust the uses of a type. The support for conjunction of true statements works like defined in the example above. A disjunction can still be represented by combining the information into the object containing several types. For a function definition, this is typically the calling function would contain the disjunction information about it's arguments.</p>"},{"location":"philosophy/basics/#annotation-applications","title":"Annotation Applications","text":"<p>The final major structure is an annotation application. These applications are used to provide additional annotations after a function is written.</p> <p>It can also be used to apply to only certain kinds of calls to the function. It is designed similarly to CSS selectors and many of the same capabilities are available. Instead of using a DOM tree, it matches against the function call tree. Here is an example:</p> <pre><code>apply sort#name(\"quickSort\") &gt; sort#name(\"selectionSort\") sort\n  #name(\"selectionSort\")\n</code></pre> <p>The main use of the applications is to provide more information towards choice. It can also be moved into a separate file to work like a configuration. Note that while annotations can give instructions to tools such as the compiler, testing, or webdocs, it doesn't change the fundamental knowledge environment.</p>"},{"location":"philosophy/basics/#other-type-system-features","title":"Other Type System Features","text":"<p>Following the above definitions, there are several interesting possibilities worth bringing attention to. These are not additional language features, but cases within the above features that are worthy of attention.</p>"},{"location":"philosophy/basics/#multiple-return-values","title":"Multiple Return Values","text":"<p>When defining a function, you can use either function overloading and/or default arguments like other languages. Unlike them, a function can accept the same arguments and return different return values.</p> <p>For example, a <code>sqrt</code> function might return <code>Optional[Number]</code> where it can only be computed if the input is at least 0. It could also return a <code>Complex[Number]</code> including the irrational component. Either of these are reasonable return values for the function.</p> <p>In Catln, these two definitions can share the same name and the desired return value will be detected during type inference. While this seems like it could have unintended effects, arrow testing guarantees that it should be safe.</p> <p>Furthermore, implicit conversions mean that multiple return values would be the norm rather than the exception. For example, a complex number with no irrational component can be implicitly converted to an int. The int can then be implicitly wrapped in an optional to form the same <code>Optional[Number]</code> anyway.</p>"},{"location":"philosophy/basics/#multiple-level-functions","title":"Multiple level functions","text":"<p>Another useful type system feature is the use of multi-level function definitions. For example:</p> <pre><code>String concat(concat(String a, String b), String c) = concat([a, b, c])\n</code></pre> <p>Since functions are tuple trees, multi level functions can be handled naturally by simply matching against multiple levels of the tree at once.</p> <p>This can be used to resolve potential performance problems like the one above. It is faster to concatenate the multiple strings all at once rather than doing them individually. Most languages have no ability to express this idea and require manual effort by the programmer. In Catln, this ensures that any way that you write your code would be optimized by the optimization rule and get the same performance.</p> <p>This feature is also very useful as the backbone of metaprogramming.</p>"},{"location":"philosophy/basics/#partial-application","title":"Partial Application","text":"<p>The idea of partial application is that function arguments don't have to be set at once. Just like tuples can be updated, the named tuples can be updated as well (including function calls). A partially applied tuple is one where only some of the keys have been given definitions and others are still missing them. For example:</p> <pre><code>baseFunctionCall = callName(arg1=1, arg2=2)\nfinalCall = match val\n    True -&gt; baseFunctionCall(arg3=9)\n    False -&gt; baseFunctionCall(arg3=2)\n</code></pre> <p>Once the value is used as the result type (determined through type inference), then the function will actually be called and the result returned. The partial application feature can be used to handle currying and function manipulation.</p> <p>One other potential usage of partial applications is to be able to call the function even when it is only partially defined as long as the missing arguments are not used. This is more common in the case of data types. For example, a circle data type given only the radius but not the center location is still sufficient to determine the circle's circumference.</p>"},{"location":"philosophy/choice/","title":"Choice","text":"<p>Choice is a fundamental problem throughout programming. However, most languages don't really acknowledge it or deal with it in any way. The general premise is simple. Every action has three parts:</p> <ol> <li>What - What do you want to do? As an example, let's say we want to sort a list</li> <li>Options - What are the options you can take? Here, it would be sorting algorithms like quicksort, mergesort, selection sort, bubble sort, etc.</li> <li>Choice - Which choice should you make and when should you make it?</li> </ol> <p>Choice is tricky. Maybe in general you could just choose quicksort. But, if you know a list is small then it might be better to choose selection sort. Or, if you know a list is almost sorted then it can be faster to use insertion sort.</p> <p>There are many other examples of these kinds of choices. If you have a value, you can choose to compute it strictly or lazily. If you have a button on a website, you can choose what background color it should get. If you have a database, you can choose whether it should be SQL or NoSQL. If you have a scatter plot, you can choose what title to put above it.</p> <p>Now, let's go into the scatter plot's title example a bit more. Imagine that your code A calls a library B which calls plot. From the perspective of the one writing the plot function, they won't have any idea what a good title is so they can only leave it blank. From the library writer's perspective, they can probably come up with a decent title but it may be formulaic or overly vague. Only from your code A will you know enough to definitely come up with a good title.</p> <p>This leads to what I call the Fundamental Rule of Choice: the ability to make better choices depends on how well you understand the context in which the choice is made. That doesn't mean choices made with little context are necessarily bad. The library might have a formulaic title, but for many use cases it will be just fine. It is really just an acknowledgment that only with full context will you know if it is fine or not.</p> <p>What separates choice from standard programming is that it crosses layers of abstraction. Normally code is supposed to obey \"Separation of Concerns\" or the \"Single Layer of Abstraction\" principle. But, obeying this would be definitively bad because it means little context and therefore poor choices. Instead, the main What code can follow separation of concerns while the Choice can combine them.</p>"},{"location":"philosophy/choice/#examples","title":"Examples","text":"<p>While it can be hard to understand choice from just a general description, let's go through a few examples.</p>"},{"location":"philosophy/choice/#css","title":"CSS","text":"<p>Probably the closest implementation to full choice is HTML and CSS. Imagine you have a function <code>render(Html html)</code>. In general, the HTML defines the what and the CSS defines the choice in how it is rendered. </p> <p>The options are fixed by the render function. They turn into CSS properties like <code>display</code>, <code>color</code>, and <code>padding</code>. As the rendering is part of the browser, only the browser creators are able to add or modify the CSS options.</p> <p>Once again, the CSS is the choice. It is composed of two parts: the selectors and the properties. Each of the selectors describes a situation, when the choices will take effect. Then, the properties are the actual choices.</p> <p>The selectors allow you a lot of control in how general or how fine the choice will take effect. It can be something general like <code>div</code> or specific like <code>div.class1 span#id2:hover &gt; textarea</code>. When multiple selectors apply, the most specific one takes effect. This is similar to the rule of choice that the choice with the most knowledge (most specific) is likely to be the best.</p>"},{"location":"philosophy/choice/#low-level-programming","title":"Low-Level Programming","text":"<p>Another important example of choice is low-level programming. Low-level programming revolves around the management of various aspects necessary to run a program on an actual CPU including memory management, caching computations, data structures, data types, hardware, etc.</p> <p>When writing high-level code, these aspects become involved during compilation. Specifically, the compiler performs metaprogramming that transforms the high-level code into equivalent low-level code that can be directly executed. And, there are choices for how that transformation takes place. The compiler can choose to be strict or lazy, choose to use reference counting or garbage collection, can choose to use array lists or hash lists, can choose to use CPU or GPU, etc.</p> <p>This behaves like the fundamental rule of choice. The general low-level metaprogramming strategy should provide a good default, but it is ultimately limited. This is the source of the performance gap between low-level and high-level programming.</p> <p>However, low-level languages resolve the gap by making programming worse. Rather than focusing on the main ideas you want to express, you also have to mix in ideas related to the low-level nature of the code in violation of separation of concern. Choice enables you to leave the main code readable and not have to specify low-level details on non-critical parts of the code. When you desire, you can still override any choices to better optimize the program.</p> <p>It also works well for the adage \"make it, work make it right, make it fast\". The \"make it work\" would be creating basic definitions while neglecting all choices. Then, \"make it right\" would be adding tests after those definitions. Finally, \"make it fast\" would be adding choices through apply statements to those definitions. Best of all, each of these steps can be done with completely separate code and there is no risk that \"making it fast\" could harm the readability or introduce new bugs.</p>"},{"location":"philosophy/choice/#implementation","title":"Implementation","text":"<p>With a few examples, now we can look into how choice actually looks like. There are two main ways that choice manifests: choice of function and choice of arguments.</p>"},{"location":"philosophy/choice/#choice-of-function","title":"Choice of Function","text":"<p>Choice of function results from the use of overlapping function definitions in Catln. This means that functions can have the same input types and function signature but different definitions. Every use of an overlapping function creates a choice of which function to use. Here is an example of creating several overlapping sort functions:</p> <pre><code>List.sort -&gt; List\n  # This creates a declaration of sort which all definitions should follow.\n    While not strictly required, it is a good practice.\n\n# Next, we will provide a definition of sorting which we will call \"quick\" for quicksort.\n\nList.sort -&gt; List =\n  #name(\"quick\")\n  ...\n\n\n# We gave quicksort a name annotation to differentiate them.\n  All sort functions share the same function name because they represent the same \"what\".\n  So, a secondary name is required to refer to one particular algorithm vs another.\n\n# As this practice is quite common, the next two use a syntactic sugar for the name annotation\n\nList.sort\"merge\" -&gt; List =\n  ...\n\nList.sort\"selection\" -&gt; List =\n  ...\n</code></pre> <p>So, when you call a function by <code>lst.sort</code>, it is really a generic or abstract call. It could refer to any of the sort definitions and which particular one is left to choice.</p> <p>A particular variation can be called using <code>lst.sort#name(\"quick\")</code> or <code>lst.sort\"quick\"</code>.</p> <p>Finally, you can use an apply statement like:</p> <pre><code>apply sort\"quick\" sort\"selection\" sort\n  #name(\"selection\")\n</code></pre> <p>The apply statement allows you to modify the sort of an existing function. This is the preferred way to write it because it keeps the main function cleaner. And, this way can work no matter how far down sort is in the call stack from you.</p> <p>Another useful option is dispatch or heuristic implementations. These implementations can provide a basic heuristic to make decisions among the function choices. As an example, here is a heuristic that might be made for choosing a sorting algorithm based on the length of the list:</p> <pre><code>List.sort = if this.length &gt; 256 then this.sort\"quick\" else this.sort\"selection\"\n</code></pre> <p>They are useful for providing low-context choices or default behaviors that attempt to leverage the type system to gain a bit of extra understanding about the context. Here, the length filter could be resolved automatically depending on the possible list lengths the type system determines. Another factor that could be added to the heuristic is whether the data type in the list is small and should be sorted directly (like ints) or large and should be sorted with pointers to the data (like strings).</p>"},{"location":"philosophy/choice/#choice-of-arguments","title":"Choice of Arguments","text":"<p>Along with the choice of functions is the choice of arguments. These work similarly to optional arguments in other programming languages. The main difference is that optional arguments are passed in only by the calling function, but choice arguments can be passed in from any part of the code regardless of how many function calls are between them.</p> <p>As an example, let's say we have the scatter plot function with a title as a choice argument. It would look like:</p> <pre><code>plot(List[Num] x, List[Num] y, Optional[String] #title Nothing) -&gt; Image = ...\n</code></pre> <p>In this example. we have the <code>plot</code> function with two normal arguments <code>x</code> and <code>y</code> and a single choice argument <code>#title</code>. Title is a choice argument because it's name begins with <code>#</code> and it has the default value <code>Nothing</code>.</p> <p>Plot can be called by adding the title annotation to the call of plot such as <code>plot#title(\"Title\")(x, y)</code> or <code>plot(x=x, y=y, #title=\"Title\")</code>.</p> <p>It can also use an apply statement like:</p> <pre><code>apply foo plot\n  #title(\"Foo plot\")\n</code></pre>"},{"location":"philosophy/choice/#quantifying-choice-outputs","title":"Quantifying Choice Outputs","text":"<p>One thing to be aware of with choice is that arrows in Catln are not equivalent to functions, either in math or other programming languages. Functions map each input to a single output, but arrows can map an input to many different outputs. One example is the plotting we saw earlier. Specifically, the arrows are equivalent to a mathematical statement and can be read as \"\u2200 input arguments, \u2203 output\". </p> <p>However, there is a special class of arrows that are uniquely quantified. They are equivalent to the stronger statement that \"\u2200 input arguments, \u2203! output\". And, these are equivalent to functions. Here is how a uniquely quantified arrow can be indicated with one example, sorting:</p> <pre><code>List.sort -&gt; List\n  #unique\n</code></pre> <p>With sorting, we do know that regardless of the sorting algorithm used, the sorted list will always be the same. The major benefit of uniquely quantified arrows is that it is possible to use arrow testing with them so it is best to indicate them when possible.</p> <p>When working with non-unique arrows, it is important to be aware of different possible outputs. The general assumption is that the arrow represents a possibly vague intent and anything that successfully fulfills that intent would be valid.</p> <p>One good way to think about this is like a journey. The arrow has a starting point and one or more ending points. Choice allows you to control everything between the start and the end.</p> <p>To control what output is returned, there are two ways. The first is to use choice directly such as the earlier example of a title when plotting.</p> <p>The second way is to use types to filter the endpoint. As an example, take computing with approximate numbers. Because working with perfectly precise numbers is difficult and often expensive, most of the time you should work with approximate numbers like floating point numbers instead.</p> <p>If you had an approximate number <code>n</code> and used it in <code>printf(\"%.2f\", n)</code>, it prints <code>n</code> with up to 2 decimal places. This means that you only need <code>n</code> to be accurate to 2 decimal places. In this case, it is possible to compute <code>n</code> faster using faster data structures (float instead of double), less accurate approximations, and more numerical instability. On the other side, requiring more decimal places would mean slower but more accurate computations.</p> <p>The way this works is that it reduces the possible output types of the arrow producing <code>n</code>. Now, the choices have to be made in order to produce the desired output type, not just any possible choices. The same thing can be used for various other restrictions on the arrow up to the capabilities of the type system. Even if you don't use something that automatically adds restrictions like <code>printf</code>, a type can be manually given to <code>n</code> to help determine what the requirements for it are.</p>"},{"location":"philosophy/choice/#visualizing-choice","title":"Visualizing Choice","text":"<p>Another important task is how to visualize choice. When trying to understand what choices are made and why, having an appropriate visualization tool would make the process much easier. For low-level programming and optimizing performance choices, it would be critical.</p> <p>Currently, I image a tool similar to the chrome inspector.</p> <p>The inspector lets you visualize the equivalent of the function execution for a particular function call (with inputs). You can expand different levels of the tree to see what sub-calls were made in each function.</p> <p>On the right, you can see two panes. The left of the two panes shows all of the \"choice selectors\" that are being applied in order of precedence. This lets you see the specificity, how they are overriding the choice, and which choices are being made.</p> <p>Finally, the rightmost pane shows all of the final choices which are made for the particular call you are looking at. The one modification is that this section would likely to be heavier on documentation and links to documentation. Unlike with CSS where the options are fixed, the options would vary heavily and figuring out what choices are available will likely be an important task.</p>"},{"location":"philosophy/choice/#design","title":"Design","text":"<p>So far, I have mostly focused on a micro view of choice that is focused on individual choices made. It is also useful to think about choice from a macro view of how all of the choices form a whole which is larger than the sum of it's parts.</p> <p>Choice, like many fundamental problems, are recognized by many people. They will separately try to solve it or part of it and often give it different names and terminologies. When I think about the macro view of choice, I tend to use one of those synonyms: design. As a synonym, it also contains the same three components:</p> <ol> <li>What = Design Requirements - What does it take to be a successful design?</li> <li>Options = Design Choices - What are the design choices that can be made? Given the design choices, it is possible to expand them out to form the full set of possible designs which is called the Design Space</li> <li>Choice = Design - The actual design work of making design choices.</li> </ol> <p>When looking at the whole of design, we first need to establish the purpose of design. Really, let's start with the purpose of not-design: the What, Design Requirements, or modeling component. The goal of the model is only to establish the truth of the problem domain. It should be organized matching the natural organization of the problem, include nuances of the domain, and attempt to promote pure understanding.</p> <p>Then, the purpose of design is to best accomplish various goals for the code. As an example, let's say we are designing a web service. One of the goals is that the service should have low latency. What makes design especially tricky is that there are often multiple goals and those goals are often contradictory. In our web service, another goal might be to have the service be low cost. But, low cost makes it difficult to still maintain low latencies so a tradeoff will have to be made between these goals.</p> <p>In fact, it is often worse. Another goal might be to have the service be robust and highly available and that is contradictory with both latency and cost. But, even something like latency is not that simple as there could be many pages and actions in the web service that each have their own latencies and choices like the database can have wide-reaching ramifications on all of them.</p> <p>Another example of design is the appearance of a website. There are a number of design goals to this such as looking nice, following conventions from other websites, not being too similar to other websites so it doesn't feel like copying, working well for disabilities and color-blindness, have information density for power users, have sparser information so it feels cleaner and doesn't confuse newer or elderly users, and all of the performance design goals. With these design goals, many of them are even subjective.</p> <p>As a final example, consider performance optimization. The three largest contradictory design goals are fast runtime, low memory usage, and low binary size. But, it often depends on the particular input given to a program as different inputs would have different performance characteristics. It is not possible to have something be optimal for all inputs. Even if you make many variations of the code and functions in it to work on different sets of inputs, it will still result in a large binary size.</p> <p>So, every design has goals and often contradictory ones. This is why humans are still important to these processes to look at the design choices, the tradeoffs that are made, and determine whether it is worth it.</p>"},{"location":"philosophy/choice/#automatic-design","title":"Automatic Design","text":"<p>Given this, a legitimate question is whether it is possible to automate some or all of the design process. It is and I think this is a valuable addition to the software development field. There are two requirements to do this successfully:</p> <p>First, there must be a way to automatically evaluate designs. While it may be theoretically possible if designs can be compared, it is best if a numeric score could be given to designs and the goal was to find a design to minimize/maximize that score. This means that weights would have to be given to the various tradeoffs to combine them into a single score.</p> <p>Second, you must establish which choices to automate. If you are trying to improve the performance of a website, you wouldn't want the appearance of the site to be changed as well. So, you need to establish which choices are subject to optimization and which ones will be fixed.</p> <p>Now, let's go through a few examples of automatic design. One of the more useful is probably a compiler strategy known as Profile-guided Optimization (PGO). One reason compiling is difficult is that it must compile without knowledge of typical inputs. PGO gives the compiler some number of sample inputs, profiles them, and uses the profile to improve the performance on those inputs.</p> <p>This can be treated as an automatic design problem. The evaluation would be the performance (runtime and memory usage). The choices to optimize would be the low-level and compiler choices. As these would be the same for most usages of PGO, it could be integrated into the compiler for easier usage.</p> <p>A more creative example would be to use automatic design to make a nice looking website. This case is interesting as evaluating a website is subjective so it can't be simply measured. Instead, it could evaluate the designs using machine learning. Then, the choices would be based on appearance such as colors, sizes, spacing, shadows, etc.</p> <p>A final example would be that machine learning itself can be treated as an automatic design problem. The training would be the automatic design and production would be using the design. In this case, both the model structure and the model parameters would be treated as choices while the loss would be the automatic design goal.</p> <p>When doing automatic design, efficiency is a large problem. Even when viewing just the restricted design spaces you are trying to optimize, it is usually too large to test each element in the space. So, it must test a random subset of designs to find the best one. Different strategies can be used and tested to determine this subset such as evolutionary methods, Bayesian optimization, or gradient descent.</p> <p>Finally, once a design is chosen it must be serialized into some format. The simplest format is to serialize into apply rules similarly to users will write and then it can be read as standard Catln code. Other imported files could also be used for large choices such as deep learning model parameters. And, the serialization would also be useful for design choices that require persistence such as website appearance or database used that shouldn't change by accident.</p>"},{"location":"philosophy/choice/#future-development","title":"Future Development","text":"<p>Right now, the algorithm I have described above for choice should be reasonably sufficient. It should work well in the general cases and can be improved over time with more rules given. Even in specific cases it can be manually controlled.</p> <p>Other languages that lack choice essentially force there to be one option. Then, the choice is always trivial. While this solution for choice isn't perfect, it is no doubt better.</p> <p>The main problem with this choice strategy is that it is essentially greedy. It assumes that all choices are independent, but they are often not. In that case, multiple choices should be made simultaneously. It can use some strategies such as adding choice costs instead of absolute choices, polyhedral optimization, or e-graphs. The only difficulty is that it also makes manual control more complicated.</p> <p>These are all areas for future development of the language. But, it requires the same semantics of choice as a background so using the simpler greedy algorithm should be a good start towards those future developments as well. Hopefully, they can even be added on to later versions of the language.</p>"},{"location":"philosophy/compilerErrorStatement/","title":"Compiler Error Statement","text":"<p>One of the other benefits of the powerful type property system is that it is possible to identify programmer mistakes as a library developer. Then, you can provide clear error statements about what the user is doing wrong. This is part of a larger trend of making it possible to describe more compiler behavior from within the language itself.</p> <p>For example, you might have a method <code>List.get(Int i)</code>. If typechecking finds that the <code>i</code> that is passed in is less than zero, there is almost certainly a mistake by the programmer. In this case, it is worth having a compiler error statement. In this instance, the compiler can give the error back to the programmer and convert the runtime error into a compile error. It might look like:</p> <pre><code>List.get(Int_lt(0) i) = compilerError(\"You attempted to access a list with an index less than zero\")\nList_length[Int_lt(l)].get(Int_gt(l) i) = compilerError(\"You attempted to access a list with an index guaranteed to exceed the length of the list\")\n</code></pre> <p>The most common use of this would be that a method is called with properties that are proven to be bad. Another case might be creating methods that allow the program to typecheck, but still throw the compilerError. This converts a vague typechecking error into a potentially more clear kind of error. One example would be assuming an optional is safe:</p> <pre><code>Optional[$T] -&gt; $T = compilerError(\"You tried to use an optional value as if it was not optional\")\n</code></pre> <p>Also under consideration, it might be worth throwing some of these as warnings instead of errors.</p>"},{"location":"philosophy/context/","title":"Context","text":"<p>One of the fundamental problems for a programming language is the management of state. In order to manage state well, there are a few goals for a successful system.</p> <p>In imperative languages, the state management is relatively straightforward by using variables and memory. However, it behaves like a blunt instrument in that it works for everything but loses too much information. It is hard to know what kind of variables, mutations, and state changes could happen in a function. It also restricts the compiler due to the limits of analyzing the control flow with the singular state. The first goal is that the state should be explicitly controlled in the type system so it is easy to understand and optimize.</p> <p>In functional languages, there isn't an implicit variable map and states have to be created explicitly. This does make programs easier to understand, but it can be verbose and unwieldy to have different values (state1, state2, state3, ...) for every change made to a state. This brings the second goal that state should be easy to update.</p> <p>The first technique functional programming languages use to manage the state, beyond the naive, is as a monad. Combined with a \"do-notation\", this saves significantly on the verbosity. However, the monad is fairly unintuitive for a full state. It converts your function into a lambda that is run using a <code>runState</code> function. This means that when combining functions, you are building up a larger and larger function to execute. This is remarkably convoluted. The simpler form of a state is that a function should accept a state as part of the input and return a state as part of the output. The third goal is that state management should be intuitive.</p> <p>The other issue with monads is that they are difficult to combine. If each monad fulfills the idea of \"do one thing and do it well\" instead of having a monolithic monad, then it must be possible to combine the monads. Unfortunately, monads can't combine. Building monoliths is also not right. If using the slightly more powerful form of a monad transformer, they can combine but it is complicated, verbose, and ultimately lacking.</p> <p>More than just combining, the combination needs to change easily. You need to add things to the state partway through the computation or prune state when it is no longer needed. This shouldn't be a side part, but one of the key aspects of state management.</p> <p>One approach which I have seen in more recent systems is effect systems. This is similar to monads, but adds more into combination and a bit into adding/removing parts of state. However, it also falls into the problem that it is based on monads instead of state changes (<code>$A -&gt; $M[$B]</code> but should be <code>$M[$A] -&gt; $M[$B]</code>) as being convoluted.</p>"},{"location":"philosophy/context/#context_1","title":"Context","text":"<p>The solution that Catln uses is called context. It corresponds to a data object defined in the standard library:</p> <pre><code>data Context[$T]($T val, $states...)\n</code></pre> <p>The context can be thought of as a collection of values, each with it's own type. The primary operations on the context are to add additional elements of state to the context and to get the elements of the state from the context. The elements in the context are all values that have types, and can be identified by the type.</p> <p>In addition, there are also some syntax sugars to make it easier to work with the context. Many methods will require a context to be called such as:</p> <pre><code># The println method requires the IO object to be part of the context when it is called\n# It then returns nothing (unit) inside of an updated context object\nprintln{IO io}(String s) -&gt; {IO}()\n\n# It corresponds to the desugared form:\nContext(val=println(String s), IO $io, $states...) -&gt; Context(val=(), IO $newio, $newstates...)\n</code></pre> <p>Within the println function, including IO inside of the curly braces means to remove the IO element from the context. Then, the new IO produced after the println operation has to be re-added to the context before returning.</p> <p>Many functions will use context elements indirectly. They do not need to access the values in the context, but they call functions that need to access values inside the context. For these, no changes to be made to the parent function. While this may seem impure in that it gives functions side effects, it really just saves a bit of typing. The true context requirements of the function and the transitive requirements are computed during type inference, so they can be displayed by the IDE or in the docs (which is really all you need).</p> <p>There is also a few additional syntax sugars for contexts:</p> <pre><code># A value can be prepended with the context to add something to a context\n# This is very useful when returning while adding context values\nx = {valToAddToContext} valInsideContext\n\n# The standard context get requires that exactly one element of that type should be in the context\n# Otherwise, it will throw a syntax error during compilation\n# This syntax stores all elements as a Collection[MyListenerType] which can be zero or many listeners\ncallWithListeners{MyListenerType... listeners}(...) = ...\n\n# Add an element to the context within a scope\n# This makes it available within the block and removes it when the block ends\nfoo =\n  with {newContextElement}\n    ...\n</code></pre>"},{"location":"philosophy/context/#uses","title":"Uses","text":"<p>The context can be used for various purposes. One example is that unlike Haskell, IO would be represented as a Context instead of a Monad. As an easy way to decide between Monads and Context, if you can have an unwrap method that discards the context and returns the value outside of the monad, it should be a context rather than a monad. Methods should return updated IO values when they accept IO as an input Context. Similarly to this, Context can be used for readers, writers, and state.</p> <p>Context can also be used to pass constant environment information down the call stack. In this instance, the value would be consumed by functions that need it and the same value would be re-added to the context for the return value. It might be useful to add a <code>const</code> keyword as syntactic sugar for the reasonably frequent cases of values that are only read from the context.</p> <p>Context could also be used to implement a listener or observer pattern. The listener event would be implemented as a type class. To add listeners, add values that implement the class into the context. The listener can be called by finding all values implementing the class within the context and calling the method on all of those values.</p>"},{"location":"philosophy/context/#implementation","title":"Implementation","text":"<p>While Context will have it's own syntax, it can actually be implemented as a generic <code>Context</code> object in the standard library (although it would be a fairly complex one with a lot of macros). It is only a syntactic sugar. This means that the variable accesses can be fully determined during type inference. It is therefore a zero-cost abstraction.</p> <p>For things such as failure state, they should not be represented using the context but by the return values. If it was represented in the context, it would be unclear which element in the context should contain the failure state.</p> <p>It should also be possible for methods to be defined with default values to be used if the desired type is not found in the context. This can improve the generality of the method but still allow for contexts to affect it.</p> <p>Alongside the context, it may also be helpful to have a syntax similar to the haskell \"do\" notation. The meaning of it can just be various forms of mapping like the original. But, it may need to support multiple kinds of maps simultaneously such as functor, monad, and also context monad map.</p>"},{"location":"philosophy/debugging/","title":"Debugging","text":"<p>Good debugging tools are essential to a good development experience. This is one of the areas where functional languages sometimes struggle. While they often have good REPLs, other things like debuggers do not work quite as well.</p>"},{"location":"philosophy/debugging/#repl","title":"REPL","text":"<p>One of the first tools that can be used for debugging is a REPL. Being able to easily execute a function with different inputs can both help during the code writing and also help check which inputs your function does not get the correct result for. A REPL would therefore be a good first item to include in terms of debugging tools.</p>"},{"location":"philosophy/debugging/#debugger","title":"Debugger","text":"<p>In most imperative languages, a debugger stops at a particular line of code and shows you all the variables in scope (and possibly in your call stack as well). You can advance your debugger to the next line of code either into a function or over a function, as well as continue to the next breakpoint.</p> <p>As the language is not time based, there is no need to limit it to only moving forwards through time. The best way is to view your whole program execution as a single giant tree. You can then explore the tree. Each node represents a function and the children are the variables and sub-functions called in the tree. You can then go down the tree to the child functions or up the tree to view the parents. As each function has all immutable values, all values can be shown simultaneously as they don't change during the course of the function.</p> <p>Instead of breakpoints, it is equally easy to simply list all of the places in the tree where a particular function is called. The user can then go through all of them as well as skip over ones that are not interesting easier.</p> <p>Another powerful option is to add a <code>#debug(val)</code> annotation. These values should be printed out when debugging begins similarly to a typical debugging print statement. However, the user can click on them to go straight to the node in the tree representing that value to combine both print statement and debugger debugging for the best value.</p>"},{"location":"philosophy/documentation/","title":"Documentation","text":"<p>Documentation is important for developing a language for maintainable code. Documentation exists as a goal in several different ways:</p>"},{"location":"philosophy/documentation/#code-documentation","title":"Code Documentation","text":"<p>Code documentation is probably the most well established. This includes documenting types, variables, methods, parameters, and return values. Documenting all of these ideas could help readers understand the code being written.</p> <p>However, there are mixed results of this. Natural documentation exists in the form of naming and type signatures. Naming can help give guidance about the relative purpose of everything. The reason Catln focuses on named tuples rather than positional arguments is precisely to increase the amount of names to add natural description to the code.</p> <p>The other factor is the usage of types. Having a precise and descriptive type system allows a significant amount of information and relationships about code to be given. At all times, it is better to prefer the compiler comprehensible use of types rather than informal ideas only passed in documentation. Due to the power of these signatures, I am hesitant to require documentation about things like methods and return values as the signature often presents that information even more clearly.</p> <p>Another tricky aspect of this documentation is the independence of these docs. In reality, many separate pieces of docs are in fact related. They could link to each other, but including would be better.</p>"},{"location":"philosophy/documentation/#other","title":"Other","text":"<p>However, it is often easy to miss the other important kinds of documentation besides the code documentation. First, it is often useful to have documentation for a package to describe what broadly speaking is located inside that package.</p> <p>Another set of documentation exists in the case of examples. Examples help describe to readers the best ways to use the code. These ideas can serve to introduce sections and to summarize them. It is also an important piece to guide users to.</p> <p>I would argue that tests are also a case of documentation. Most tests that are written should be property tests and arrow tests. That leaves the remaining cases of integration test and unit tests. An integration test should behave similarly to an example, otherwise it doesn't have a lot of utility in terms of describing behaviors users are not expected to do. Unit tests should exist to explain and enforce important properties of the code. If those properties are not even worth describing to readers, they are not likely important enough to deserve a test. See testing for more details.</p> <p>Finally, there are also documents to explain the cohesive theory behind the code. For others who are learning about it, there are many important descriptions of not just what code exists but why the code was created that way. Without a clear location for this information, much of it is only known by the developers who work on it and eventually lost.</p>"},{"location":"philosophy/documentation/#documentation-in-catln","title":"Documentation in Catln","text":"<p>In Catln, I think of documentation as fundamentally intertwined with the idea of code organization. Imagine that you were writing a book about your library. You want to organize it in the clearest order to explain each of the key concepts. For your book to be sufficiently precise, it would essentially need to include the code to help formalize all of the ideas described. If your book also has all of the code, then it means that your code could be entirely found within this documentation book.</p> <p>One of the trickiest parts about documentation is to keep it in sync with the code. While no system is perfect, I would argue that the most important aspect is to have them kept as close together as possible. It is far easier to recognize bad method documentation than a document nobody ever sees.</p> <p>In this book format, each file should correspond to one \"page\" of documentation. A section of the book would then be a subdirectory or package of code. Unlike a book, these sections will probably be a thinner and deeper tree (fewer chapters but more hierarchy in each). Each directory would have a main file that would provide a description for the section of the book as a whole and can also include the order of the files/subdirectories within.</p> <p>In reality, the book would also exist on two levels: public and private. The division is pretty simple that everything not marked private (see modules) would be in the public book and both public and private alike would be in the private book. For this reason, it will be possible for documentation sections, tests, and examples to all be marked private.</p> <p>The purpose of the public book is for consumers of the library. They are only interested in the details necessary for them to use it and information not likely to help them should be marked private. The private book should be directed as maintainers of the library who need to understand additional details. Because this should be a strict superset of all the details a consumer would need, all the public sections should be just as relevant in the private book.</p> <p>In addition to the main book, there will also be collated pages for various scopes. These would include all of the methods on the scoped type, functions in the scope, and a list of subtypes with their relevant subscopes. Each of these sections would link back to their place in the book combining the key explanations in a desired order with a better ease of reference. This section is more similar to the documentation that exists now like Javadoc or Doxygen.</p>"},{"location":"philosophy/documentation/#format","title":"Format","text":"<p>In a particular file, it consists mostly of documentation/comment blocks and code. The code is written as expected. All comments will be treated as documentation in the markdown format. This can be either a single line or an indented block. The comment can be prefixed with the private keyword to mark the section as private. Code blocks within the markdown will be assumed to have catln code and will be treated the same as an example.</p> <p>A documentation section should be separated with newlines before and after if it is an independent section. If the documentation is specific to a particular definition, it should be located directly before the definition without a line break separating the two. These documentation pieces will then be collated into the type summary documentation as the accompanying description.</p> <p>It is also possible to have these comments and documentation sections within the bounds of a function definition.</p> <p>For example sections, they will be put within an <code>#example</code> annotation block. The examples will always be type checked, however they will also have several kinds of additional syntax. It can replace sections of code with a <code>...</code> and miss key definitions. It can also use values without defining them. If an example is runnable, then it would be run as part of testing. If not, it will still be parsed and type checked to the greatest possible extent to help ensure that the examples are kept up to date. Within the same file, all examples will be considered as in the same code file (alongside the actual code). So, values created in one example can be used in the others. Values will have to be named to avoid conflicts.</p> <p>Similarly, tests will be inside a <code>#test</code> annotation block. Tests must be runnable without particular syntax. A test behaves the same as any other definition or declaration. They are run using the same mechanism as arrow testing. The only key difference is that they will show up highlighted in documentation and will be filtered out during compilation.</p>"},{"location":"philosophy/documentation/#visit-the-stack-documentation-to-see-a-live-example","title":"Visit the stack documentation to see a live example","text":""},{"location":"philosophy/languageCompilation/","title":"Language based Compilation","text":"<p>In order to have an effective compiler system, I would argue that as much reasoning should be pushed into the language as possible. The primary reason is that the compiler is relatively fixed. It can only be improved periodically while changes can be easily made to the code that is run.</p> <p>The other key difference is that the compiler only has a relatively small knowledge about the program. It is possible to understand some basic domains of code like numbers, strings, and lists, but it would be much more limited working on all of the custom types that are created. While many optimizations can be made without this knowledge, plenty require it. The problematic requirement has typically been that there is no way for the language to really reason about the code. In Catln, the type properties provide a sufficient reasoning ability for much of the necessary things that compilers typically do. All that should be left are more fundamental operations based on the language system itself.</p> <p>The other direction is within the area typically thought of as compiler plugins. It should be possible to develop these within the bounds of the language itself as well. Here are some of the key \"plugin\" directions I envision:</p>"},{"location":"philosophy/languageCompilation/#optimization-rules","title":"Optimization Rules","text":"<p>Many rules for optimization can be embedded within the language itself. While many optimization rules are strictly based on the structure of the code, others require knowledge of the types themselves. The largest way to accomplish this is through multiple level functions.</p>"},{"location":"philosophy/languageCompilation/#imports","title":"Imports","text":"<p>One important feature of a file of code is to interact with external sources of information. This description sounds a bit vague because I believe it has far more power than it typically used by languages. The most typical usage is a standard code import.</p> <p>The standard code import would have to either be built-in or part of the standard library. It should allow importing another Catln file into the scope of the current file. Remember that the language consists of essentially objects and arrows. So, the objects and arrows of the other file are brought into the current file.</p> <p>In addition, I may add an additional import type for dependency Catln code. These differ from a standard import in terms of the strictness. Bad formatting or warnings shouldn't be allowed within the main code, but could be excused in dependencies. They would also be excluded from coverage metrics and it may be worth skipping tests directed at dependencies as well.</p> <p>Beyond this, the same idea of \"importing\" could be used as a general system of creating objects and arrows as prerequisites for the code in the file. For example, a static json file could be \"imported\" into the program as a JSON object. Then, the contents of the file can be used while verifying through the type system that the code matches the file. It could also import things like a text file as a string or a static image to return from a web server. These can then be optimized out during build time to use these external sources of information just as if they were Catln code directly.</p> <p>Another powerful option could be to import types themselves. For example, the OpenAPI specification allows you to design an API as a JSON file. If that file were imported as a type, then the type checker could ensure that the implementation matches the API definition. Not only would it simplify the code, it would help prevent bugs and errors. The same could be used for other code generation like protobuf files.</p> <p>Another avenue for importing is to handle language interop. For the simplest example of C header files with static binaries, the functions in the C header could be imported as objects. However, the arrows would all be undefined within the actual Catln except by interoperating with the code generation.</p> <p>Likewise, a similar system could be used to interop with other languages as well. Swift for Tensorflow added Swift &gt; Python interoperability to take advantage of the expansive library of python functions. The same could be used to leverage the python libraries from Catln too. However, it would be better to create wrappers as the Python language lacks the same degree of type expressiveness.</p> <p>Overall, an expandable import system would help resolve the requirements on code generation/macros from the build system while providing high ease of use to developers. It would most likely require heavy use of the macro system, but that should be fine.</p>"},{"location":"philosophy/languageCompilation/#compiler-targets","title":"Compiler Targets","text":"<p>Potentially the most powerful area is by changing the compiler targets. It is standard to think of a program producing a single binary executable. In the current design, this executable is produced by building the system into a tree form, generating the tree into LLVM, optimizing the LLVM, and saving using LLVM.</p> <p>However, there is another way to consider it. The main function of the program has the following signature <code>main{IO io} -&gt; IO</code>. It we create a type <code>CatlnLLVMExecutable</code>, the final compiler stages could be described as metaprogramming <code>llvm(c=runnable{IO io} -&gt; IO) -&gt; CatlnLLVMExecutable</code>. This function could be written within the standard library instead of the compiler.</p> <p>While it is possible that the code will be more difficult, it shouldn't be too large of a difference. It shouldn't be crazy given the power of implicit conversions, multi-level functions, and macros and will end up fairly similar to simply writing it in catln to begin with.</p> <p>There are a number of benefits to this approach. First, it means that the compiler becomes extendable and additional libraries can interact with the compiler types and classes. By using abstract choices, whole compiler modules can be replaced wholesale if desired. The compiler can also be configured at a high granularity using the same systems as annotation configuration files that specify the how details of the code.</p> <p>Another benefit is that it is possible to write multiple compilers. Instead of the standard LLVM compiler, a simple direct to x86 compiler could be made that might run faster for use during development. It can also be used to create a separate parallel compiler or even a GPU compiler to replace the standard one.</p> <p>The greatest power is expanding the very nature of a program. The final result type can be expanded from a single executable to a more general <code>CatlnResult</code> class. Then, we would have <code>every CatlnLLVMExecutable isa CatlnResult</code> so that the LLVM binary would be one valid result from compiling a Catln file.</p> <p>Other results could be generated as well. For building a server, maybe it would generate the server executable along with a database schema. If you are building a full-stack web application, it could generate both the client and server simultaneously. You would only need one program and one language for your application, while being able to write validation to apply on both sides and type checking to help ensure your client and server match. You could even throw in mobile as well.</p> <p>It would also be possible to program distributed architectures. Then, Catln could choose (or be given) the architecture for your application and automatically integrate them into a final result package. Instead of writing the business logic within the distributed code, they can be written completely separately, utilize a common interface, and be combined later. It might even be possible to switch out different architectures to find the best one to suit your application. Beyond that, it could even generate a full cloud system as a specification for a cloud provider (AWS, GPC, Azure, etc.) written in the cloud configuration (CloudFormation, Terraform, etc).</p>"},{"location":"philosophy/macros/","title":"Macros","text":"<p>While having strong strict typing rules can greatly reduce bugs and help guide code, you eventually run into limits. The only way to deal with these limits is to resort into short circuiting the type system using features like casting, repeating a lot of code, or you will have to start implementing macros.</p> <p>I try to differentiate the terms metaprogramming and macros. Metaprogramming is used for programs or functionss that accept and modify the code from other functions before executing them. Because they often fit naturally in the existing language constructs, they don't need special handling (see metaprogramming).</p>"},{"location":"philosophy/macros/#reflective-functions","title":"Reflective Functions","text":"<p>Instead, macros have two types. The first type is used to refer to arrow definitions which are more unusual in terms of what they match. They may accept variable argument names or variable numbers of arguments.</p> <p>Consider as an example the <code>zip</code> method. This method takes a tuple of lists into a list of tuples. A single <code>zip</code> method with fixed tuples can be created normally, but to support any kind of input tuple (or more than two input lists to zip together at once), you would need to resort to macros.</p> <p>Then, the macros are implemented using reflective functions as opposed to infinite functions. So, it is not possible to create <code>function zip${N}</code> for all <code>N</code> because that would require creating infinite objects. Instead, create a single object <code>zip</code> that would apply for all numbers of things to zip.</p> <p>The main thing that differentiates this from a normal function is that it may use special internal functions. For example, functions usually have fixed arguments. Some could have arbitrary arguments, but not inspect them. The most common would probably be functions to inspect or work with arbitrary number of arguments. Others could be those that look into how data structures are used or access information about the global set of objects.</p> <p>Macros should be computed only during compile time. If they can change during runtime like lisp, they can be used to create infinite functions and then the only things callable within them are other macros. Restricting to compile time means that there are no performance costs and all of the same time checking and type inference will still work. It can even be used to create \"pseudo-source code\" which can be included in the documentation for the used versions of the macro.</p> <p>I intend to use the dollar sign as the key to macros including macro types, macro values, and the special functions limited to macro context. Here is some sample pseudo code for how macros might look:</p> <pre><code>zip(List $args...) =\n    Int size = $args.$dict.map(v -&gt; v.size).values.min\n    List(size=size, values=(i -&gt; $args.$map(v -&gt; v.get(i))))\n</code></pre>"},{"location":"philosophy/macros/#code-generation","title":"Code generation","text":"<p>In other words, the first type corresponds to a single (fancy) arrow. It can correspond to an infinite number of arrows without those expanded capabilities, but reduces that infinite number down to one. The second type of a macro lets you generate multiple arrows, but a finite number. The most common usage of this is for defining imports.</p> <p>As an example, say we are importing a C file. Each function and type in that C file can be added as additional objects and arrows in Catln. But, as the C file is finite, it would only generate finite things into the Catln environment.</p>"},{"location":"philosophy/metaprogramming/","title":"Metaprogramming","text":"<p>Metaprogramming is writing code that treats other code as it's data. While problems that require metaprogramming are not that common, lacking strong capabilities when they are needed can make it very difficult. There are also many problems that can take advantage of metaprogramming to be simpler and more applicable.</p> <p>To motivate this discussion, let me give a few examples of metaprogramming problems:</p> <ul> <li>Simple optimization rules such as <code>++(String l, r=++(String l=rl, String r=rr)) = concat([l, rl, rr])</code>. These rule should apply to all code that is written and can help resolve what would otherwise be performance problems. These can help users focus efforts entirely on readability without needing to also worry about performance, but can be especially critical when working across function boundaries.</li> <li>Auto-differentiation. This takes a function with numerical input(s) and a numerical output and figures out the derivative of the input(s) with respect to the output. For deep learning, this is an important task because finding the effect of various model parameters on the output accuracy is the main input to model optimization. It requires applying differentiation to all of the code used in the model to figure out how each called function works and combining them all to find the overall differentiation.</li> <li>AutoCloud. One of the goals in Catln is to create a metaprogram function <code>autoCloud(webApplication)</code> that will take a web application designed for a single server and produce an output designed to work across a cloud provider such as AWS. It would need to synchronize database accesses and local state and make use of various services such as lambda/beanstalk for computation or cloudfront for serving static content. It can also include best practices such as metrics, logging, deployment, and migrations.</li> <li>Compilation. In Catln, the fundamental compilation task of moving from high-level Catln code to the low-level executable (llvm but could also be raw assembly) is done using metaprogramming. In fact, building an executable is defined as just <code>llvm(c=main)</code>. First, this helps move work from the compiler itself to the standard library to make it easier to create new versions and improve. It also makes the low-level features such as strict/lazy or memory management strategies adjustable using choice. New compilation areas such as specialized code for compiling highly parallelizable N-dimensional data can be added by just importing an additional library. Finally, it helps bridge the gap between high-level and low-level programming by ensuring code can live anywhere in between.</li> </ul> <p>The goal of metaprogramming is mostly the same as programming. It should be easy to write and easy to read. The type system should help prevent bugs and mistakes. There needs to be testing, examples, and documentation. In my mind, the largest goal is to help bridge the gap between normal programming and metaprogramming to make metaprogramming just a particular case of normal programming.</p>"},{"location":"philosophy/metaprogramming/#catln-meta-programming","title":"Catln meta programming","text":""},{"location":"philosophy/metaprogramming/#tree-structure","title":"Tree structure","text":"<p>There are a number of reasons that make Catln unusually effective at metaprogramming. The first is the consistent tree structure. As all data and functions are formed from the typed-named tuples, all data naturally has this tree structure.</p> <p>Function definitions can be thought of like rewrite rules that match against a certain tree pattern and replace it with a new tree. Other features such as internal function definitions and internal value definitions are just syntactic sugar for tree rewrite rules. They are duplicated to create the function tree and then later de-duplicated during compilation with a Common Subexpression Elimination pass. This means that the metaprogramming can also be described with these same kinds of tree rewriting rules. Even cases where values must be reused such as IO are desugared into anonymous interior functions that share the values using function argument insertion.</p> <p>In an imperative language on the other hand, you also have to deal with other structures such as memory, variables, loops, and control flow. You would need to track what variables exist and where they were created. In compilers, they usually convert these to an immutable form called Single Static Assignment (SSA) just to manage the complexity. But, metaprogramming has to either operate without effectively managing the complexity on the raw text form or after converting it into the SSA form most users would not be familiar with.</p> <p>A pure immutable tree on the other hand can effectively manage the complexity as is. Variable reuse is only done through arguments. Conditions and branching only happens when deciding which function rewrites to apply (after desugaring). Loops only use higher order functions or recursion. And, the metaprogramming would just apply recursion to the recursion which should just work.</p>"},{"location":"philosophy/metaprogramming/#homoicionic","title":"Homoicionic","text":"<p>The next reason is that the language, like lisp, uses an identical structure for both data and functions. This property is known as homoiconicity. This means that all of the functions and familiar tools within the language can be used to manipulate the functions as are used to manipulate the data.</p> <p>Many languages have relatively weaker homoiconicity where they have some kind of Code data type. Then, code is just converted into a code data type or code is executed. In Catln, functions have arguments applied one at a time like data, be used in a pattern matching expression, or assembled into classes of functions. You can even have type properties for a function data type.</p> <p>Lastly, it makes it easy to operate not just on a particular function call but on the data being given to the function call. So, you could have metaprogramming rules that use only part of a functions input rather than the whole thing.</p>"},{"location":"philosophy/metaprogramming/#quasi-quoting","title":"Quasi-quoting","text":"<p>Unlike lisp, Catln does not require the use of quoting and unquoting. When the structure of code and data is the same, quoting is used to inform the compiler when something should behave as code and when it should behave as data.</p> <p>While this seems reasonable, the process doesn't scale. It will work when one metaprogramming is being applied at once, but breaks down if multiple need to be integrated. For example, you may want to compile your program but still apply all of the simple optimization rules as well.</p> <p>Instead, Catln uses the type inference process to determine when functions should be applied. This includes both normal functions and metaprogramming ones. Because of this, it is much easier both to call metaprogramming and integrate metaprogramming with normal programming.</p>"},{"location":"philosophy/metaprogramming/#choice","title":"Choice","text":"<p>This does leave Catln compilation to behave in a way which is unpredictable and almost nondeterministic. However, problems are fundamentally nondeterministic with multiple possible solutions. Instead of running away from this and trying to force determinism, it is much simpler to leave it naturally non-deterministic. Then, provide the tools to effectively manage the nondeterminism.</p> <p>This concept is referred to as choice. In the area of metaprogramming, I tend to think of it as changing the goal from defining what should be done to what can be done. One of the most difficult things is not the process of applying various rules, but to know whether they should be applied to begin with. Choice allows these two problems to be completely separated. It makes both tasks significantly easier to understand and implement. Most importantly, it is able to grow to add new possibilities afterwards. So, metaprogramming doesn't need to worry about the nondeterminism and can rely on the choice to resolve it.</p> <p>Choice is a problem that exists everywhere in programming, but some of the most troublesome areas are in metaprogramming. This is due to the fact that metaprogramming combines one domain of knowledge with an arbitrary domain of knowledge. Usually, the the calling domain knows what it is calling. But here, neither domain knows anything about each other.</p> <p>Consider the autoCloud from single server example. Let's say that somewhere in the single server code, which I will refer to as the business logic, you have a map applied to a list. There are actually three ways this can be handled. First, the map can be applied in a single thread. Second, the map can be applied by multiple threads on a single machine. Third, the map can be divided across an entire cluster of machines in the cloud. The metaprogrammer writing the autoCloud won't know anything about the business logic so they won't know what map to use. The business logic shouldn't know about the cloud to know what map options are available. So, there is no place to put the desired type of map without choice.</p> <p>Another example of this is that it is also not necessary to distinguish during metaprogramming which code should be executed during compile time and which should be executed during runtime. I am not saying that this is not a problem, but that this is not a problem specific to metaprogramming. The same thing applies to other code such as pre-computing constant values, constant branching, or pre-computing a <code>printf</code> expression with a constant string. Instead of having a partial solution that only applies to metaprogramming, it is better to leverage a more global solution with choice. So, the metaprogramming should be abstract over this choice as well.</p>"},{"location":"philosophy/metaprogramming/#type-system","title":"Type system","text":"<p>Through all of this, one final note is that the metaprogramming code still behaves quite naturally. It all uses the same type system that helps prevents errors. All the code is still valid code, not arbitrary strings, and would just use anonymous functions for inserting arbitrary behavior. It follows a normal format for testing, documentation, and examples. And, it doesn't even require special syntax or features.</p>"},{"location":"philosophy/metaprogramming/#metaprogramming-format","title":"Metaprogramming Format","text":"<p>In Catln, the mechanism for meta programming is a multi-level function definitions (see basics). The usual format of application is <code>metaprogrammingFunction(function(functionArgs...))</code>. Depending on the task, some metaprogramming tasks can be implemented with just standard definitions in this fashion. Others will need harder to represent arrows that use the specialized macro capabilities.</p> <p>This is also interesting because it falls naturally within Catlns model of computation. Imperative programming and functional programming are both limited by the idea that code is always evaluated. Metaprogramming is essentially that code can be more than just an evaluation target. So, input arguments that are multi-level and contain metaprogramming are part of the \"tuple substitution\" nature of the language. There is no need to change it or expand it to handle this problem. This is why metaprogramming fits with the standard programming and the existing type system.</p>"},{"location":"philosophy/modules/","title":"Modules","text":"<p>In all languages, a module system is important to divide code into reasonably sized chunks and to ensure you are working with the desired chunks. It is also essential for the sake of naming, because otherwise names would either have to be overly long or risk conflicts. This is especially true when combining different libraries that lack knowledge of each other.</p> <p>In Java for example, each file corresponds to a single class. This is only reasonable considering it's verbosity. In reality, the behaviors of a type or class are just as likely to exist in the intersection of types as focused mainly within a single type. Even once the ability to add classes separately from the type declaration, the methodology falls apart. However, the association of methods to their most relevant class is very effective from the perspective of both documentation and discoverability.</p> <p>Haskell instead allows for a more free-form organization of codes. Modules can be imported either directly or qualified. However, only documentation can say which one is the right way to import for a given module. It is also difficult to discover functions without digging aimlessly through documentation. However, the organization better suites the problem domains because it is based freely on the cleanest way to view code.</p>"},{"location":"philosophy/modules/#type-and-class-scoping","title":"Type and Class Scoping","text":"<p>In Catln, functions can be attached to types and classes. Then, they are called such as <code>list.append(v=5)</code>. However, this is actually just a syntactic sugar over a direct function call <code>List::append(this=list, v=5)</code>. So, it doesn't require much additional effort at the language level besides some desugaring and special handling for docs/error messages. </p> <p>The <code>::</code> syntax is used to modularize functions within the scope of a type or class. It can also be expanded in several ways. A function can be within a scope without requiring a dot such as <code>List::empty</code>. This makes it similar to a static method. The main benefit of scoping a function is to add a context to the function that separates it from similar methods in different contexts like <code>Graph::empty</code>. It also allows for the construction of documentation pages featuring all of the methods within a type or class scope. These pages will be some of the more useful ones when programming with a library. Because of this, the code files should not necessarily be organized based on the scoping (see documentation).</p> <p>Another extension of the syntax is to hold other types and classes within a scope. For example, you could create the type <code>List::Array</code> or <code>List::Linked</code> which implements the <code>List</code> class. This also helps differentiate naming for types and classes as well. For example, an <code>Env</code> type would be useful in many different contexts.</p> <p>Scoping can also have multiple levels. So, values like <code>List::Array::empty</code> can be created and organized with respect to it's full scope as well as <code>List::Linked::empty</code>.</p> <p>For this reason, it should also be possible to have a root scope. The base of a scope would be prefixed with a <code>::</code> such as <code>::List</code> similar to directory paths with <code>/</code>. This helps clearly differentiate an absolute vs. relative scope for a value.</p>"},{"location":"philosophy/modules/#adding-to-a-scope","title":"Adding to a Scope","text":"<p>Any value can be created with a scope by simply specifying a larger scope in the definition:</p> <pre><code>Optional::empty = Nothing\n\ndata List[$T]::Cons = Cons($T head, Cons[$T] tail) | Empty\n</code></pre> <p>It is also possible to add a scope to an entire code block by using an extension:</p> <pre><code>extend Optional\n  empty = Nothing\n  ...\n</code></pre> <p>All definitions, types, and classes created within the indented area would be automatically prefixed with the given scope. This is better suited to creating many definitions within the same scope.</p>"},{"location":"philosophy/modules/#calling-with-a-scope","title":"Calling with a Scope","text":"<p>The other difficulty with having a scope is having to describe which scope values and functions are in. If you always give the entire scope, it would be overly verbose. This would discourage the creation of a sufficiently large scope hierarchy to effectively organize all of the ideas.</p> <p>Another option is to allow some values to be called without the scope or with only part of a scope. However, it then makes it difficult to keep track of when values require scope and when they don't. It tends to encourage overly long names to avoid the use of scopes and relatively robotic choices about importing values scopeless.</p> <p>Catln proposes a new solution to the problem of scoping: type inference. Packages and files are imported by the current file making everything available within them also available in the current file as if they were all written in the same file. Even within an extend block, it should only affect the declarations and creation of new functions and types, not the usage of existing ones.</p> <p>Instead, you could simply use a value such as <code>empty</code>. It would then be up to the type inference to use the context in combination with the rest of type inference to decide what the correct scoping is. Because you did not specify it with an absolute scope (<code>::empty</code>), it is registered as a relative scope. From the type system, it can then be treated as a combination of the possible scopes matching the name. However, it must be discerned down to a single scope before type inference ends otherwise it throws an \"unable to infer scope\" compiler error.</p> <p>If such an error is thrown, the value can be replaced with one that has more scoping information. For example, that could be replaced with <code>List::empty</code> which would be inferred with the only possible resolution of <code>::List::empty</code>. The other nice part of this system is that using the type inference, it is unlikely that such scoping values would be required often. Even when they are, only the most convenient values or functions could be scoped to enable the rest to be better inferred.</p>"},{"location":"philosophy/modules/#private-definitions","title":"Private Definitions","text":"<p>Another important piece is how to make data, parameters, functions, types, and classes that end users should not be using. Not having privacy makes it difficult to maintain compatibility without a clear scope of what can be safely changed. It can makes the picture overly messy to end users by showing them things they are likely not interested in.</p> <p>However, I am against strict privacy bounds. When types can be extended after creation and by different packages, it is important to be able to pierce the bounds when necessary.</p> <p>Catln features a private keyword that is prefixed to various declarations to mark them private. So, there are no impacts on the naming based on privacy. However, the privacy exists only up to a scope. When declaring a method within the same scope or a subscope of the desired private value, it can be used just like any other. Outside of the scope, I currently plan to require the use of absolute scoping. So, the values would be more verbose to underscore the peculiar usage of a private value. It will also be possible to specify a specific scope following the private keyword and to have a private indent area.</p> <pre><code>private List::Array::empty = ...\n\nprivate[List]\n  ...\n</code></pre>"},{"location":"philosophy/naming/","title":"Naming","text":"<p>One of the most important aspects of readable code is good naming. Some amount of naming is difficult and requires good use of the vocabulary of the written language (e.g. English). However, the design of Catln is also intended to make naming better as well.</p> <p>One of the first ways naming is improves is through the pervasive use of named tuples. While normal tuples may require slightly less code, they don't provide names to help explain what each of the arguments is. While this information might sometimes be guessed, names should be able to add further description to it. This ensures that whenever you are working with something where a name could be useful, there is an opportunity to provide one.</p>"},{"location":"philosophy/naming/#default-names","title":"Default Names","text":"<p>Just as important as the presence of names is the absence of them. For example consider the function <code>~(Bool val) -&gt; Bool</code> that negates a boolean. Here, the name <code>val</code> is not presenting any information to the reader. To handle this case, I plan to add a new feature of type default names. So, you could instead write the signature <code>~(:Bool) -&gt; Bool</code> that does not features a name. Within the definition, you would use the default name attached to the <code>Bool</code> type instead. That type might be something like <code>bool</code> or <code>b</code>.</p> <p>By using the default names, it is not necessary to come up with names that do not present information. By using type aliases, even usages of the same type for different purposes can also utilize grouped names. This name would then be easy to change using a code refactoring tool as all usages correspond only to the default name.</p>"},{"location":"philosophy/naming/#name-guidance","title":"Name Guidance","text":"<p>Another key piece of guidance for names is that they usually consist of a single word or idea. This is not an absolute rule, but consider if multiple words can be broken down into different pieces.</p> <p>For example, instead of using an <code>ArrayList</code> type, it should use the power of modules and instead be <code>List/Array</code>. Then, it is possible to refer to it by the full name if necessary or an abbreviated name if that sufficiently describes the type. There is also no need to worry about conflicts. There could be different types named <code>Listener</code> in different scopes that would be separated by the module name, instead of the object name. No need for smurf programming.</p> <p>The other direction is to avoid having types inside names such as <code>toString</code>. Instead, it would be better to use the real type for linking, discoverability, and simplicity. Here, you could instead write <code>to[String]</code> as the function name.</p>"},{"location":"philosophy/optimization/","title":"Optimization","text":"<p>It is typical for languages and their respective compilers to be able to perform various optimizations. These optimizations allow users to write code that is more focused on readability than performance, while still gaining the equivalent performance. It can also greatly simplify code by reducing the amount of work necessary to implement various optimizations.</p> <p>One key focus of this language which differs from others is that optimizations should not be predictable. For example, consider strict (data is always computed immediately) vs lazy (data is computed only when it is needed). Most languages are predictable whether they will implement something lazily or strictly. Often, this is done by only supporting one of the two (C is always strict) or by requiring a special indication to deviate from the standard (Haskell is lazy unless told otherwise). The result of these simplistic decisions is that very often users must accept the burden themselves. C users have to manually implement laziness any time in which it could improve work. But, it is more likely that they simply would accept the easier and worse performing option of not bothering to implement it. In Haskell where users can specify a choice, it usually results in duplicate implementations where one is strict and the other is lazy. Users must therefore choose between the two implementations and the doubled maintenance costs.</p> <p>The solution to this problem is that some heuristic should be implemented to decide between the options in this use case. Any simple heuristic is likely to be bad so the only result is to use a complicated heuristic that users would not be able to easily predict the result of. Given this, users would have to check the result in order to determine what the heuristics resolved. This process could be eased with a dedicated compiler query that would return the results of this heuristic. Users trying to optimize performance could then look at the results and then change the values that they don't agree with compiler annotations. The heuristics could also be configurable to determine how much to prioritize competing goals such as performance and binary size. These could be used to tune the output results to better suit the specific application needs.</p> <p>One requirement to improve these heuristics is that users must be able to express large amounts of information about their computations. For this reason, the ability to express metadata about computations should be a focus of the syntax and the implementation. These include both rigid bounds (this number must be less than 128) and flexible bounds (the list usually has a small number of elements).</p> <p>Below, I will discuss a number of these kinds of optimizations that should be implemented in order to avoid passing on the implementation cost to the users writing in the language. Note that the focus of this is high level optimizations. It is assumed that the language will be converted into a lower-level IR and be passed through another optimization system (currently LLVM) that applies unmentioned lower-level optimizations as well.</p>"},{"location":"philosophy/optimization/#inlining","title":"Inlining","text":"<p>One of the most basic examples of this case is inlining. If you inline a function, it will increase the performance by removing the calling overhead. When the function is only used in one place, it also will reduce the binary size a bit. If the function is inlined into multiple places, it would increase the binary size by replicating the function in multiple places. It is most beneficial when the function is small and the calling overhead consists of a large amount of the total cost of the function. When it is large, the calling overhead makes little difference. Another potential criteria is that it can make optimization more difficult for the downstream compiler because larger functions require more work to optimize.</p>"},{"location":"philosophy/optimization/#strict-vs-lazy","title":"Strict vs Lazy","text":"<p>Results of computation can either be strict or lazy. If they are lazy, they are computed only when needed which can save the overhead of computing values that are not needed. When those values require significant computing time, this could be a large savings. It can also be used in two additional ways. It can actually change the algorithmic complexity of a computation such as the famous example of quicksort with laziness (O(N^2)) being equivalent to quickselect (O(N)). Lastly, it can be used in the case of infinite computations such as infinitely sized lists. These can then be used to simplify other computations and have spread outside of just the functional world such as python generators.</p> <p>Strictness also has a number of benefits. It has no overhead in terms of performance cost or memory usage while laziness does. This can be an especially large problems in cases such as inserting elements into a map where there are large numbers of small functions that could all be deferred. Using strictness will reduce the overall performance of the application by a significant margin. For this reason, strictness should be considered the default behavior and laziness only used when the benefits outweigh the costs.</p> <p>Laziness should be used whenever something has infinite size. In the case of a conditional where some values are only used by one branch of the condition, those values should be made lazy if they require a non-trivial amount of work to compute (greater than laziness overhead). Note that this also applies to the properties of an object where some properties could be lazy while the whole is not. As another potential improvement, values can be lazy in tandem where multiple will all be accessed or none will be accessed. Combining those could further decrease the performance overhead.</p>"},{"location":"philosophy/optimization/#caching","title":"Caching","text":"<p>Another important optimization is the use of caching to prevent values from being recomputed. However, caching also has an overhead in terms of performance (checking if a value was cached) and memory usage (storing a potential value). It also greatly increases the memory usage by preventing the cached values from being freed.</p> <p>One potential use of caching is associated to objects. For example, the length of a linked list could be associated with it. If the length is commonly queried, then it makes more sense to cache it. This would also mean computing it recursively as the linked list is created. Storing the cache as part of an object can help for determining memory usage because the cached values share the same scope with their associated object.</p> <p>Another important area of caching is general memoization. Consider an incremental compiler as an example. Using memoization, it would be trivial to avoid recomputing operations that are isolated such as removing syntactic sugar from a function or typechecking it. With memoization, this performance improvement comes for free as long as the compiler runs as a daemon or server where the memory can be preserved. Otherwise, it requires specific intention to use.</p> <p>Another more specific use case is dynamic programming. This involves a recursive function (or mutually recursive functions) that combine to compute a single value. It can be improved using standard memoization, but specific handling can result in better memory usage. It is also worth noting that some of these dynamic programming problems are also universal (such as fibonacci) and could be reused, so it shouldn't be local only.</p>"},{"location":"philosophy/optimization/#unused-values","title":"Unused Values","text":"<p>For an object, it is possible that the values on it can be unused. For example, a linkedlist can contain a head pointer, tail pointer, forward pointers, and backward pointers. Depending on how it is used, it is quite possible for some of these pointers to not ever be needed. In that cases, those pieces of data can be pruned from the object to remove the memory usage and computation cost.</p> <p>Another potential avenue for this is values that can be determined at compile time. For example, the length of hashsum can be known based on the type of sum and neither needs to be stored nor computed.</p>"},{"location":"philosophy/optimization/#remove-levels-of-abstraction","title":"Remove levels of Abstraction","text":"<p>It is often useful to have wrapper types such as Optional which help enable sum types. In this case, it should not have a cost to use these types over not having them to avoid discouraging good usage of abstraction. For example, if the optional is guaranteed to return a value, it should be equivalent to not having an optional at all.</p>"},{"location":"philosophy/optimization/#type-selection","title":"Type Selection","text":"<p>Another potential area for optimization is the selection of types. For example, users must pick between the different list variations including linked lists, array lists, skip lists, deque, trees, and hashmaps. Languages then have to choose between which implementations are worth supporting and how many can be supported before it merely confuses users. Some languages simply provide one option with the idea that it is sufficient.</p> <p>For this reason, it should be important to allow the compiler to choose between a number of implementations of an interface. This would also require some way for the implementer to provide guidance such as avoid this implementation if it calls this function. It can also take advantage of profiling through the unit and integration tests to determine how the interface is typically used. Users could also specify an actual value. But, even a poor heuristic from the compiler is likely to be better than users choosing without any particular thought. See more.</p>"},{"location":"philosophy/optimization/#method-specialization","title":"Method specialization","text":"<p>When there are multiple possible implementations of a function, there may not always be a best option. For example, sorting with quicksort is better for large arrays while sorting with selection sort can be faster with small ones. This can be used to improve the overall performance. This is also handled as a choice abstraction.</p>"},{"location":"philosophy/optimization/#memory-management","title":"Memory Management","text":"<p>Memory management is one of the most important optimizations in a language. Systems generally fall into manually managed memory (C++, Rust), Garbage Collection (Java), and Reference Counting (Swift, Python). While having memory managed by systems like this can greatly reduce bugs and simplify development, that is only if it works well enough. A bad management system does not leave you any better off.</p> <p>For memory management, the primary system should be passed on lifetime. Consider your functional program as a tree. Values are created and can then be propagated up the tree. However, there should exist a level on the tree where the value is guaranteed to be removed and propagated up no farther. By doing this analysis, the values can be freed without requiring any runtime analysis and they can be allocated and freed efficiently in a combined group as well. This technique can be supplemented by reference counting as well.</p> <p>Another component for analysis is the use of stack vs heap. Using the stack can help improve the memory usage as well when it is possible.</p>"},{"location":"philosophy/optimization/#parallelization","title":"Parallelization","text":"<p>One of the key benefits of functional programming is that it can greatly simplify parallelization. By having pure code, it guarantees that sections must be independent and therefore can be computed independently in different threads. However, diving the work up into the threads requires a little bit of work. Functions should be divided up into chunks which can each be run in parallel. Then, they can be added into a global queue and multiple lightweight threads can each pull elements off the queue and then execute them. If chunks depend on other chunks, they will check if all chunks have finished after working and then add the successive chunk code to the queue at the end.</p> <p>Chunks should be large enough to avoid contention for the queue and not spend too much time outside of chunks. Then can include one or several subtrees that will be arguments to a larger tree. For large parallel operations (like a map), they should be divided into chunks with a fixed size that is determined based on the code in the map and then the remainder can be combined with the last chunk.</p> <p>While this works for thread parallelization, there is also GPU style parallelization. For this, it can be focused on specific data types that typically work well with GPUS. It may also require a special context to enable GPU support as well.</p>"},{"location":"philosophy/optimization/#only-subset-of-function-needed","title":"Only subset of Function needed","text":"<p>Sometimes, the application may not need all of the functionality provided by a library. For example, if it only calls the function with one argument set to true, it would be more optimal to remove that argument from the function and substitute the value into it. This requires additional compilation but has only benefits during runtime.</p>"},{"location":"philosophy/optimization/#promises-and-futures","title":"Promises and Futures","text":"<p>One area to consider is how to manage asynchronous code including promises and futures. For most of the code which should be pure, this shouldn't be a problem because the general parallelization should be able to handle it. Some structure may be needed for IO and the monads that are order dependent.</p>"},{"location":"philosophy/optimization/#static-and-dynamic-dispatch","title":"Static and Dynamic Dispatch","text":"<p>One avenue worth considering is static vs dynamic dispatch. Static dispatch can be faster although it requires duplicating the code for each of the possible input types (binary size). Dynamic dispatch requires another layer of indirection before making calls but does not duplicate methods. The best option might be a combination of the two depending on the requirements and the goals. For example, only use static dispatch for the hot loops where it would greatly increase performance.</p>"},{"location":"philosophy/optimization/#pre-computing","title":"Pre-computing","text":"<p>There are often expressions or functions in a program which can be computed during compile time instead of run time. Essentially, anything that doesn't use IO could be computed. However, it is hard to determine which ones are reasonable to compute and which ones might require too much work. But, pre-computing the right tasks such as with constant strings in a <code>printf</code> could drastically speed up the program.</p>"},{"location":"philosophy/syntacticSugar/","title":"Syntactic Sugar","text":"<p>A good language should come in two forms: a minimalist form and a usable form. When optimizing, converting, and analyzing code, it helps to reduce the amount of language features to the bare minimum. While the ideal of having a simple to learn language is also good for end users, various additional kinds of language features and syntax could be adopted that greatly simplify coding in the language. Therefore, the easiest thing to do is have a number of language features that are converted into their minimalist equivalent and then used in that way. Here are a number of important examples of features that should be added as syntactic sugar. </p>"},{"location":"philosophy/syntacticSugar/#closures","title":"Closures","text":"<p>One of the standard ideas with higher order functions is the closure. A lambda function uses the environment not of it's execution, but of it's creation. This enables values to be passed into the lambda from the external code. However, preserving these environments is bad for memory usage and makes analysis difficult. Instead, I will convert lambdas that accept values from the environment into currying. By checking within the lambdas to see which variables must come from the outside, those will be passed into the function and the function will gain those as additional arguments. Then, the lambdas can be treated as a top-level function and doesn't require any special handling as well. Because currying is already a useful feature and can be accomplished with partially applied tuples, it can reasonably handle this form.</p>"},{"location":"philosophy/testing/","title":"Testing","text":"<p>For writing secure, reliable, and maintainable software, testing is a necessity. It can be used to check your code does not have bugs and to prevent bugs from appearing when updating and refactoring the code. However, testing is also the subject of numerous blog posts, debates, and discussions. They argue the relative merits of unit tests and integration. The importance of test coverage, mocking, and test driven development. I will describe my current thoughts on testing and how I plan to implement it with Catln here.</p>"},{"location":"philosophy/testing/#testing-fundamentals","title":"Testing Fundamentals","text":"<p>Think of each function as having a domain, co-domain, and a map from each element of the domain to an element of the co-domain. The code function is then an algorithm that produces this relationship. Tests provide a redundant mapping from domain to co-domain to be used to check if they match.</p> <p>A unit test and an integration test represent specific elements in the domain and manually verify they produce the correct items in the co-domain. So, it will test only a few inputs to the function. Ideally those inputs are chosen with purpose. In unit tests, the purpose is variety. In integration tests, the purpose is that it matches real use cases.</p> <p>However, it is unreliable for humans to decide the appropriate elements from the function domain to test. There are other alternatives. The most common alternative is known as property testing. In property testing, you describe a property of the function that should be true for all inputs. For example, a property of appending to a list is that the length of the list increases by one. This property is true, and could be verified, for all types of lists, lengths of lists, contents of the lists, and elements that are being appended. Then, the testing Engine can accept the responsibility of choosing items from the domain to test as well as ensuring code coverage.</p> <p>The one potential issue with property testing is that the choice of properties may not fully verify correctness. In the example of appending while only checking the length, numerous incorrect (if contrived) problems are possible such as inserting the wrong element, messing up the rest of the list, and changing the types. None of these would cause the property test to fail. Formally, the property testing restricts the co-domain and then verifies that the values produced are in the smaller co-domain. A co-domain of 1 would perfectly test the values, but larger co-domains will not. In the appending example, we could perfectly property test it by ensuring that the last element is the one appended and the preceding elements are the original list. Regardless, property tests greatest strength is how little work is required to create them.</p> <p>Another potential avenue for testing is by creating multiple definitions of a function. Those definitions should then match for all inputs in a domain. Duplicate functions are the best form of testing as they can fully verify across the domain and fully verify each element in the domain. This idea is a part of arrow testing.</p>"},{"location":"philosophy/testing/#testing-implementation","title":"Testing Implementation","text":"<p>Writing tests requires only a few simple operations: the test/example annotation, the assertion, and a testing engine. The testing behaves most similarly to property testing so any function that has assertions inside it will be subject to testing.</p> <p>You can put the assertions directly into the implementation of functions for property tests. If you want to test specific cases (unit or integration tests), you should create a global value for the test, give it a <code>#test</code> (or <code>#example</code>) annotation, and include the necessary assertion inside it. For having tests that are randomized, just use a global function instead with the same annotation and the randomized inputs as arguments.</p> <pre><code>testAddDouble(Int x) = \n  #test\n  assert(test=x + x == 2 * x)\n</code></pre> <p>You can also create a simple unit/integration test case by simply creating an overlapping function definition. For example, you could write:</p> <pre><code>#test\n  operator+(1, 1) = 2\n</code></pre> <p>If you have duplicate functions, no additional work is necessary. They will be run automatically as arrow tests.</p>"},{"location":"philosophy/testing/#testing-engine","title":"Testing Engine","text":"<p>In the actual testing process, all tests really align into two ideas. Any function which has an assertion should be verified by property testing. This is used to execute all of the complicated unit/integration tests too.</p> <p>Then, any test with overlapping definitions is executed using arrow testing. The simple tests written as sample function inputs and outputs just use arrow testing (with the domain of one item) to match against the other definitions.</p> <p>One requirement of this system is the ability to create sample function inputs. If you come from an imperative background, this may seem somewhat unreliable. Generally, that means that you are not making a strong enough use of the type system to encode the fundamental ideas behind your data types. You should write it so that any invalid combination of data would not be allowed in the object and match the type definition. The reverse of this is then any type creation would be a valid input. Furthermore, all of the edge cases should be much easier to create as there are either boundaries in numbers, or cases in the sum types.</p>"},{"location":"philosophy/testing/#test-proofs","title":"Test Proofs","text":"<p>In lieu of using the testing engine to produce sample inputs to a function, it may be possible to prove using the type properties that an assertion is guaranteed to be true. This would actually be proven during type inference, so in those instances no tests need to actually be run. You can also specify that an assertion must be proven for a stronger verification.</p>"},{"location":"philosophy/testing/#test-caching","title":"Test Caching","text":"<p>In order to have a good development experience, it is important for the tools to be as fast as possible. That makes it easier and more convenient to run them more often. This is also an important piece in testing. With the heavy use of arrow testing and property testing, the tests might be generally slower than most testing systems.</p> <p>In order to improve the performance of the testing, the tests can be cached. If no input functions to a test have changed, the tests do not have to be rerun and the result can be pulled from the cache. Given this, it is far faster to run an entire test suite without worrying about wasting time.</p>"},{"location":"philosophy/testing/#testing-difference-engine","title":"Testing Difference Engine","text":"<p>With testing, one goal is to avoid accidentally changing the behavior of a function. To suit this goal, it requires an extensive test suite with high coverage. This test suite requires significant manual work to create and upkeep. Instead, a tool can do it almost effortlessly.</p> <p>This powerful addition to the testing system is the use of a testing difference engine. The difference engine expects you have a copy of the code in two different files (or directories). It tests the two implementations against each other using arrow testing to ensure that matching functions should be equal for all possible inputs. When it isn't, the engine should (as always) attempt to find the simple examples that demonstrate how it changes.</p> <p>The most useful application of this difference engine is for testing over time. If you have a pull request, you can view the difference before and after the changes. This will help the reviewer identify what functions have changed and how. Without this, it often requires that those changes are not systematically detected due to no tests or that the PR author manually updates all of the relevant tests to the new definition. Instead, the use of the difference engine merely requires enabling a tool.</p>"},{"location":"philosophy/testing/#coverage","title":"Coverage","text":"<p>When the tests are run, it will automatically record the code coverage as well. The coverage will be recorded using only a single metric of blocks (essentially conditionals covered) instead of by line. For each block, it will also be recorded whether the test was complete (arrow testing or perfect property testing) or incomplete (unit testing or imperfect property testing). It will also record which tests cover each line of the code as well.</p>"},{"location":"philosophy/testing/#guide-to-writing-tests","title":"Guide to Writing Tests","text":""},{"location":"philosophy/testing/#unit-testing-integration-testing-and-examples","title":"Unit Testing, Integration Testing, and Examples","text":"<p>One of the first, and largest, areas of testing is unit testing. Some plan for many unit tests while others consider them too tedious. Overall, I believe that there are often better tools to accomplish the goals that unit testing often handles.</p> <p>First, unit testing is sometimes used to ensure correct behavior on edge cases. For this purpose, an effective type system should be better. With a bounded sum type, it is impossible to forget to handle one of the cases without a type error. Likewise, by making properties that are optional clear in the type system, they also can't be missed. Instead of relying on manual tests, automatic types require no work and will do a more thorough and faster job of ensuring that most corner cases are handled.</p> <p>Another goal is to get good coverage. With the testing difference engine, the coverage is not as essential of a metric. Furthermore, using property tests is a much more effective way of increasing the test coverage</p> <p>In the documentation page, I describe the way of organizing code as if you are writing a book to explain the details of your library/service. In that respect, you should write manual tests for cases that you believe have explanatory value to your readers. They should illustrate useful cases to understand how the function works and usually be co-located with either the function or it's declaration. As such, there is not a lot of difference between unit tests, integration tests, and examples which is why I have combined them together.</p> <p>The only major difference would be the use of mocking. As a pure language (outside of IO), you should not need to use mocking besides a MockIO implementation. The MockIO should be reserved for only the functions that interact with the user directly while most of the logic should be located elsewhere.</p>"},{"location":"philosophy/testing/#arrow-testing","title":"Arrow Testing","text":"<p>Arrow testing should be the most convenient form of testing as it requires little additional thought. Most of it would be automatic based on definitions.</p> <p>The only time to consider arrow testing is when you are designing algorithms with thought of their runtime. While you obviously want to create the faster versions, also consider writing the simpler native version to better explain the function. This then doubles as an arrow test which will verify the more complicated implementation as well.</p>"},{"location":"philosophy/testing/#property-testing","title":"Property Testing","text":"<p>The bulk of your testing should be done in property testing. However, the bulk would only be in terms of value instead of cost. When you write functions, try to include assertions as well. These can be in terms of the final value or intermediate values as well. The assertions can also provide a useful avenue to help provide additional explanation to those reading your function as well.</p>"},{"location":"philosophy/typeTheory/","title":"Type Theory","text":""},{"location":"philosophy/typeTheory/#existing-systems","title":"Existing Systems","text":"<p>In most functional languages, the type theory is based on lambda calculus. The idea is that a function accepts a single type and then returns a new type. Functions of multiple types can be represented through currying, that it accepts the first type and then returns a new function that requires one fewer types to return the final result. This makes it naturally well suited to higher order functions.</p> <p>The problems occur once the type theory starts adding additional ideas. For sum types, they are handled by creating functions such as <code>Leaf :: v =&gt; Tree v</code> that create the sum types. While this works in general, it also hides a number of limits. It makes it difficult to handle method overloading and default arguments in methods. Product types can be handled by creating tuples. However, the result of this is that there exists the conversion of currying: <code>a -&gt; b</code> vs <code>(a,b)</code>.</p> <p>The other major extension is adding generics. They are typically considered as existential qualilfiers similar to <code>\u2200a. a -&gt; a</code>. However, this has now introduced an entire dimension of kinds into the type system for methods.</p>"},{"location":"philosophy/typeTheory/#catln","title":"Catln","text":""},{"location":"philosophy/typeTheory/#objects","title":"Objects","text":"<p>Catln has a simpler view of types. As all values in Catln are a tree of named tuples, a type is simply a set of possible named tuples. For example, an object is the basic set of tuples. A class is a union of the sets of it's component objects. A parameterized type is basically just a helper for creating sets given a type. Even type properties are simply subsets of the type they are associated with. Any construct for types merely serves as ways of building different sets of tuples, forming a simple and easily manipulated paradigm.</p> <p>One key difference of using sets is that the fundamental operations are different. In functional programming, the sum type and product types are the key operations for working with types. In Catln, it is really a union and intersection of the set types. While cross products are also important, they are naturally handled due to the named tuple format.</p> <p>For unions, they are often done using classes. For example, here is a simple union:</p> <pre><code>class AlphaNumeric = Letters | Numbers\n</code></pre> <p>This says that the sets of letters and the sets of numbers combine to form the sets of AlphaNumeric. Functions from the AlphaNumeric type therefore need to handle the entire set as the domain of the function.</p> <p>Intersections are done by applying multiple pieces of information to a single type. Here is an intersection using two different type properties:</p> <pre><code>List_len(5)_min(3)\n</code></pre>"},{"location":"philosophy/typeTheory/#products-and-sums","title":"Products and Sums","text":"<p>The sum and products can be easily constructed from this backbone. For a product type, the named tuples suffice. A pair could be constructed like:</p> <pre><code>data Pair[$A, $B]($A a, $B b)\n</code></pre> <p>A sum type could be built using the union of types with unique objects:</p> <pre><code>class Maybe[$T] = Just($T val) | Nothing\n</code></pre> <p>Unlike a pure sum, it is also possible to have overlapping types. Consider this alternative definition of Maybe:</p> <pre><code>class Maybe[$T] = $T | Nothing\n</code></pre> <p>This implementation of maybe simplifies a lot of usage by not having to write the verbose <code>Just</code> everywhere. It is also not a Sum type. In a sum type, it would be possible to represent <code>Maybe[Maybe[$T]]</code>. However, this simplified definition implies that <code>Maybe[Maybe[$T]] = Maybe[$T]</code>. I believe this is reasonable to the idea of Maybe and, in cases where you would want to differentiate <code>Nothing</code> from <code>Just[Nothing]</code>, you should really be using a custom type instead where names could be applied to describe the two forms of Nothing.</p>"},{"location":"philosophy/typeTheory/#arrows","title":"Arrows","text":"<p>Along with objects, Catln also has arrows. An arrow is a rewrite rule that automatically converts one type into another based on need. It is inspired by Scala's implicit conversions. The idea of an implicit conversion is that it represents a conversion that has a clear definition. Which implicit conversions should be taken can be inferred based off the methods that are called on the result of the conversion.</p> <p>Which implicit conversions should be taken and when should be found during type inference. The functions called on values serve as hints for when implicits are necessary.</p> <p>However, this can also result in problems if conversions happen unexpectedly or it converts to the wrong type. In theory, a well typed program should produce the same result regardless of the order of implicits as long as the implicit combination allows you to reach the desired type. This property is guaranteed through the use of arrow testing. Therefore, most issues that implicit conversions can cause should be found and they can generally be assumed to be safe.</p> <p>This also makes handling more complex language features quite simple. Each arrow definition can correspond to many possible edges in the type graph. First, it can match multiple values for a single argument such as an integer argument. Arrows can also match classes or multiple arrows for each object within a class can be equivalent to an arrow for the entire class. Even type properties can simply be considered as nothing but arrows that apply to sets of nodes. The arrow definition can even match up to a generic. Lastly, arrows can also substitute to be applied to a subtree of the main tree at a single node in the graph. So, all fancy language features essentially convert down into the two simple representations of objects and arrows.</p>"},{"location":"philosophy/typeTheory/#arrow-typing","title":"Arrow Typing","text":"<p>The set of values shows a lot of power when considering arrows/functions as well. Each arrow has an input and an output type, which can also be considered the domain and co-domain in set terminology.</p> <p>A union of two arrows would go from the union of the domain to the union of the co-domain. This is often used for class arrows. If you define an arrow <code>f(Just[$T] val) -&gt; Bool</code> and <code>f(Nothing val) -&gt; Bool</code>, the union of those arrows would be the arrow <code>f(Maybe[$T] val) -&gt; Bool</code>.</p> <p>The intersection of two arrows would go from the intersection of the domain to the intersection of the co-domain. As an example, consider overlapping definitions or declarations:</p> <pre><code>sort(List lst) -&gt; List\nsort(List_len($x) lst) -&gt; List_len($x)\nsort(List_min($x) lst) -&gt; List_min($x)\n</code></pre> <p>If you provide multiple definitions with the same name, they are assumed to be multiple definitions of the same function. Therefore, they should return the same result given the same input. However, the return types that are given are not necessarily precise. While it must contain all possible values, it could also contain values that are not really possible for the function to return. Because all definitions apply, everything in all the return types apply. In set terminology, the true(r) return type is the intersection of the co-domains.</p> <p>In the case above, it describes how several type properties are affected by sorting the list. The main declaration of the sorting function works across the entire domain of lists. The supplementary definitions only apply when the input type to the sort function is more precise than lists in general. If you know the input list length, you can then figure out the output list length. If you don't know the input list length, then the arrow does not take effect and you won't know anything about the output list length. The same applies to the minimum element, as well as both properties in combination.</p>"},{"location":"philosophy/typeTheory/#full-graph","title":"Full Graph","text":"<p>The concepts of objects and arrows can be combined into a unified directed graph structure. The objects are nodes and the arrows are edges.</p> <p>Each definition can be thought of as a graph in this form. This definition itself becomes a composable relationship in the graph. In addition, there are some additional assumed arrows such as going from a set to a superset as well.</p>"}]}